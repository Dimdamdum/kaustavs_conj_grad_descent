{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8655ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCalc is adding 2 + 3\n",
      "The computed sum is 5\n",
      "\n",
      "Quack! My name is Donald\n",
      "Quack!\n"
     ]
    }
   ],
   "source": [
    "# examples with classes\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store data in this instance\n",
    "    \n",
    "    def add(self, a, b):\n",
    "        print(f\"{self.name} is adding {a} + {b}\")  # Access self.name\n",
    "        return a + b\n",
    "\n",
    "# Usage:\n",
    "calc = Calculator(\"MyCalc\")\n",
    "summy = calc.add(2, 3)  # Python automatically passes calc as self\n",
    "print(f\"The computed sum is {summy}\\n\")\n",
    "\n",
    "class Duck:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store the duck's name\n",
    "    \n",
    "    def introduce(self):\n",
    "        print(f\"Quack! My name is {self.name}\")\n",
    "    \n",
    "    def quack(self):\n",
    "        print(\"Quack!\")\n",
    "\n",
    "# Usage examples:\n",
    "donald = Duck(\"Donald\")\n",
    "donald.introduce()\n",
    "donald.quack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9597f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1.009999999975, -1.9900000000026317, 4.99000000001, 3.1300000000159236], cost function = 52.8596\n",
      "x = [0.005897630008671893, -0.2904937204018134, 0.13457109604084116, 0.001668087453219903], cost function = 0.026028579594744642\n",
      "x = [0.00037449115354536817, -0.16550934894657934, 3.226168500859898e-05, 3.5483309608153823e-12], cost function = 0.0007721943108706685\n",
      "x = [5.4498576897916806e-05, -0.11256823123990456, 2.5927969375248464e-12, 7.114101782273717e-33], cost function = 0.00016282688486587434\n",
      "x = [1.097225479559197e-05, -0.08172503814583272, -3.4529096537761817e-28, 1.122277847165345e-55], cost function = 4.496092716242442e-05\n",
      "x = [2.5850164314183053e-06, -0.06122064525186035, 4.1293257701407547e-51, 2.185807862584084e-78], cost function = 1.4115670458544878e-05\n",
      "x = [6.622443599310257e-07, -0.04663116760110854, 5.564185457147956e-74, 1.3905268387082256e-101], cost function = 4.743645877179094e-06\n",
      "x = [1.777910982729993e-07, -0.03585051583371858, -6.917299300598905e-97, -5.502812919536767e-124], cost function = 1.655735660473815e-06\n",
      "x = [4.90287009399437e-08, -0.027709462286658784, -1.278435433395997e-119, -6.441845560901762e-147], cost function = 5.90595590587046e-07\n",
      "x = [1.3732527485209696e-08, -0.021483398961054796, 1.345962948601753e-142, 8.115096883895434e-170], cost function = 2.133303908361589e-07\n"
     ]
    }
   ],
   "source": [
    "# gradient descent basic example.\n",
    "# Not clear how \"non-analytical\" functions can get until torch stops working...\n",
    "# copilot claims torch ONLY uses analytical stuff, even for eigenvalues...??\n",
    "# general goot practice: prefer torch functions\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#possible cost functions\n",
    "def cost_function_1(x):\n",
    "    return x[0]**2 + x[1]**4 + (x[0] + x[1])**6 + x[2]**2 + x[3]**2\n",
    "\n",
    "def cost_function_2(x): # with eigenvalues involved\n",
    "    # Reshape x into a matrix\n",
    "    A = x.view(2, 2)  # If x has 4 elements\n",
    "    eigenvals = torch.linalg.eigvals(A)\n",
    "    eigenvals_real = torch.real(eigenvals)\n",
    "    return torch.sum(eigenvals_real**2)\n",
    "\n",
    "def cost_function_3(x):\n",
    "    return max(x)\n",
    "\n",
    "d = 2\n",
    "cost_function = cost_function_1\n",
    "# define variable to be optimized\n",
    "x = torch.tensor([1.0, -2.0, 5., 3.14], dtype=torch.double, requires_grad=True)\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 10000\n",
    "optimizer = torch.optim.Adam([x], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(x)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"x = {x.tolist()}, cost function = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 1000\n",
      "Step # 2000\n",
      "Step # 3000\n",
      "\n",
      " The optimized unitary is \n",
      " tensor([[ 0.6920+0.1455j,  0.5108-0.4890j],\n",
      "        [-0.3043-0.6383j,  0.6986+0.1092j]], dtype=torch.complex128) \n",
      " \n",
      "Absolute values of its entries: \n",
      " tensor([[0.7071, 0.7071],\n",
      "        [0.7071, 0.7071]], dtype=torch.float64) \n",
      " \n",
      "Compare to Hadamard: \n",
      " tensor([[ 0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]], dtype=torch.float64) \n",
      " \n",
      "The optimized block spectrum is \n",
      " tensor([0.3000, 0.3000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function\n",
    "\n",
    "\n",
    "n = [0.2, 0.4]\n",
    "lamb = 1\n",
    "# optimal rotation is e.g. a hadamard, yielding maximally mixed block spectrum [0.3, 0.3]\n",
    "# so optimal M should be s.t. M_to_A(M) = log(hadamard). here is one possibility:\n",
    "# M_optimal = torch.tensor([[ 0.4601,  0.0000], [-1.1107,  2.6815]], dtype=torch.double, requires_grad=True)\n",
    "\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized: here are some choices\n",
    "# M = torch.tensor([[0., 0.1], [0., 0.]], dtype=torch.double, requires_grad=True) # start with U0 close to identity\n",
    "# M = M_optimal\n",
    "M = torch.rand(2, 2) - 0.5 * torch.ones(2,2)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 3001\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized unitary and block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "H = torch.tensor([[1., 1.], [1., -1.]]/np.sqrt(2), dtype=torch.double)\n",
    "print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"Absolute values of its entries: \\n {torch.abs(U_best).data} \\n \")\n",
    "print(f\"Compare to Hadamard: \\n {H.data} \\n \")\n",
    "print(f\"The optimized block spectrum is \\n {b_best.data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4883994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 100\n",
      "Step # 200\n",
      "Step # 300\n",
      "Step # 400\n",
      "Step # 500\n",
      "Step # 600\n",
      "Step # 700\n",
      "Step # 800\n",
      "Step # 900\n",
      "Step # 1000\n",
      "The numerically optimized block spectrum is \n",
      " [0.60126569 0.52562717 0.49598848 0.47878389 0.3906772  0.52562717\n",
      " 0.49598848 0.47878389]\n",
      "Compare with the conjectured value: \n",
      " [0.60127 0.52563 0.49599 0.47878 0.39068 0.52563 0.49599 0.47878]\n",
      "Norm of difference: \n",
      " 1.877551628332819e-09\n"
     ]
    }
   ],
   "source": [
    "d = 8\n",
    "n = np.random.rand(d)\n",
    "lamb = 5\n",
    "learning_rate = 0.01\n",
    "n_steps = 1001\n",
    "\n",
    "# get conjectured bbest\n",
    "n_conj = nK(n, lamb)\n",
    "\n",
    "# build cost function\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized\n",
    "M = torch.rand(d, d) - 0.5 * torch.ones(d,d)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized (unitary and) block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "b_best_sorted = torch.cat([\n",
    "    torch.sort(b_best[:lamb], descending=True).values,\n",
    "    torch.sort(b_best[lamb:], descending=True).values\n",
    "]).detach().numpy()\n",
    "# print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"The numerically optimized block spectrum is \\n {b_best_sorted}\")\n",
    "print(f\"Compare with the conjectured value: \\n {n_conj.round(5)}\")\n",
    "norm_diff = np.linalg.norm(b_best_sorted - n_conj)\n",
    "print(f\"Norm of difference: \\n {norm_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e52b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.2, 0.6]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 300\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-12\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40000002 0.39999998]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.529284621679259e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39999996 0.40000004]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 5.6811281232156754e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4 0.4]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.878662781005383e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39999998 0.40000002]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.2880658023019095e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4 0.4]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.878662781005383e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function, get_b_best\n",
    "\n",
    "n = [0.2, 0.6]\n",
    "lamb = 1\n",
    "b_best_conj = nK(n, lamb)\n",
    "U_best, b_best_num, H_best, conjecture_holds = get_b_best(n, lamb, N_init=4, N_steps=300,learning_rate=0.01)\n",
    "np.allclose(b_best_conj, b_best_num, rtol=1e-6) and conjecture_holds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0645e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.2229409  0.05596309 0.1386509  0.80645666 0.91983109 0.81623408\n",
      " 0.30484322 0.99087029 0.57684608 0.68341568 0.45366076 0.33017196\n",
      " 0.80776949 0.85441828]\n",
      "  lambda = 10\n",
      "  rand_range = 1.0\n",
      "  N_init = 1\n",
      "  N_steps = 2000\n",
      "  learning_rate = 0.005\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/1\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "Gradient descent, step # 1000\n",
      "Gradient descent, step # 1100\n",
      "Gradient descent, step # 1200\n",
      "Gradient descent, step # 1300\n",
      "Gradient descent, step # 1400\n",
      "Gradient descent, step # 1500\n",
      "Gradient descent, step # 1600\n",
      "Gradient descent, step # 1700\n",
      "Gradient descent, step # 1800\n",
      "Gradient descent, step # 1900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/1\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Conjectured b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Norm of difference: \n",
      " 4.8088887725453555e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.0658141036401503e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Conjectured b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Norm of difference: \n",
      " 4.8088887725453555e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.0658141036401503e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n"
     ]
    }
   ],
   "source": [
    "d = 14\n",
    "lamb = 10\n",
    "n = np.random.rand(d)\n",
    "N_init = 1\n",
    "N_steps = 2000\n",
    "learning_rate = 0.005\n",
    "eps = 1e-13\n",
    "b_best_conj = nK(n, lamb)\n",
    "U_best, b_best_num, H_best, conjecture_holds = get_b_best(n, lamb, N_init=N_init, N_steps=N_steps,learning_rate=learning_rate, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3479a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 2, lambda = 1\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.24289581 0.33596876]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Conjectured b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Norm of difference: \n",
      " 1.5803773493345781e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Conjectured b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Norm of difference: \n",
      " 5.7553152157123256e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Conjectured b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Norm of difference: \n",
      " 6.674769602417389e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Conjectured b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Norm of difference: \n",
      " 2.944569060556366e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Conjectured b_best: \n",
      " [0.28943228 0.28943228]\n",
      "Norm of difference: \n",
      " 5.7553152157123256e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.83934664 0.0624811 ]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45091387 0.45091388]\n",
      "Conjectured b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Norm of difference: \n",
      " 4.6151766010626826e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45091386 0.45091389]\n",
      "Conjectured b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Norm of difference: \n",
      " 1.8401790725858764e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Conjectured b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Norm of difference: \n",
      " 5.97405225398519e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Conjectured b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Norm of difference: \n",
      " 4.927206845467243e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Conjectured b_best: \n",
      " [0.45091387 0.45091387]\n",
      "Norm of difference: \n",
      " 5.97405225398519e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.92092619 0.77213926]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.84653273 0.84653272]\n",
      "Conjectured b_best: \n",
      " [0.84653273 0.84653273]\n",
      "Norm of difference: \n",
      " 1.26207407527789e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.84653273 0.84653272]\n",
      "Conjectured b_best: \n",
      " [0.84653273 0.84653273]\n",
      "Norm of difference: \n",
      " 1.262391704982428e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.5543122344752192e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.84653273 0.84653273]\n",
      "Conjectured b_best: \n",
      " [0.84653273 0.84653273]\n",
      "Norm of difference: \n",
      " 9.976131993627674e-12\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6653345369377348e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.84653272 0.84653273]\n",
      "Conjectured b_best: \n",
      " [0.84653273 0.84653273]\n",
      "Norm of difference: \n",
      " 6.554836912392557e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.771561172376096e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.84653273 0.84653272]\n",
      "Conjectured b_best: \n",
      " [0.84653273 0.84653273]\n",
      "Norm of difference: \n",
      " 1.262391704982428e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.5543122344752192e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.23408168 0.86644446]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Conjectured b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Norm of difference: \n",
      " 1.3788584944102903e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55026306 0.55026308]\n",
      "Conjectured b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Norm of difference: \n",
      " 1.03735098091611e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55026308 0.55026306]\n",
      "Conjectured b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Norm of difference: \n",
      " 1.1117207496594307e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Conjectured b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Norm of difference: \n",
      " 2.0254224118875957e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Conjectured b_best: \n",
      " [0.55026307 0.55026307]\n",
      "Norm of difference: \n",
      " 1.3788584944102903e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.99046438 0.82774954]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Conjectured b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Norm of difference: \n",
      " 6.698053681613121e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.886579864025407e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Conjectured b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Norm of difference: \n",
      " 2.3956290173975867e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -6.661338147750939e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Conjectured b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Norm of difference: \n",
      " 1.1605840837948858e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.5543122344752192e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Conjectured b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Norm of difference: \n",
      " 1.439027891630503e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6653345369377348e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Conjectured b_best: \n",
      " [0.90910696 0.90910696]\n",
      "Norm of difference: \n",
      " 2.3956290173975867e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -6.661338147750939e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.90161398 0.11876549]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.51018974 0.51018973]\n",
      "Conjectured b_best: \n",
      " [0.51018973 0.51018973]\n",
      "Norm of difference: \n",
      " 6.647849896185419e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.51018969 0.51018978]\n",
      "Conjectured b_best: \n",
      " [0.51018973 0.51018973]\n",
      "Norm of difference: \n",
      " 6.080694598076883e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.327471962526033e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.51018972 0.51018974]\n",
      "Conjectured b_best: \n",
      " [0.51018973 0.51018973]\n",
      "Norm of difference: \n",
      " 1.475766579276715e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.51018973 0.51018973]\n",
      "Conjectured b_best: \n",
      " [0.51018973 0.51018973]\n",
      "Norm of difference: \n",
      " 8.339681122250466e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.51018974 0.51018973]\n",
      "Conjectured b_best: \n",
      " [0.51018973 0.51018973]\n",
      "Norm of difference: \n",
      " 6.647849896185419e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.93068749 0.32473378]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62771063 0.62771064]\n",
      "Conjectured b_best: \n",
      " [0.62771064 0.62771064]\n",
      "Norm of difference: \n",
      " 9.813192719036955e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62771063 0.62771065]\n",
      "Conjectured b_best: \n",
      " [0.62771064 0.62771064]\n",
      "Norm of difference: \n",
      " 1.3189311426435139e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62771065 0.62771062]\n",
      "Conjectured b_best: \n",
      " [0.62771064 0.62771064]\n",
      "Norm of difference: \n",
      " 1.6616035705364545e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.1102230246251565e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62771063 0.62771065]\n",
      "Conjectured b_best: \n",
      " [0.62771064 0.62771064]\n",
      "Norm of difference: \n",
      " 1.4979269113413517e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62771063 0.62771064]\n",
      "Conjectured b_best: \n",
      " [0.62771064 0.62771064]\n",
      "Norm of difference: \n",
      " 9.813192719036955e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.54440266 0.39024308]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Conjectured b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Norm of difference: \n",
      " 3.589587379017855e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Conjectured b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Norm of difference: \n",
      " 1.1901515546967524e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Conjectured b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Norm of difference: \n",
      " 1.1121538753650856e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Conjectured b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Norm of difference: \n",
      " 1.5001258807829872e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Conjectured b_best: \n",
      " [0.46732287 0.46732287]\n",
      "Norm of difference: \n",
      " 3.589587379017855e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.41475168 0.05090842]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Conjectured b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Norm of difference: \n",
      " 2.1573415802657408e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Conjectured b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Norm of difference: \n",
      " 2.9207133701308065e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Conjectured b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Norm of difference: \n",
      " 4.5908898810055434e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Conjectured b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Norm of difference: \n",
      " 5.134771498412217e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Conjectured b_best: \n",
      " [0.23283005 0.23283005]\n",
      "Norm of difference: \n",
      " 2.1573415802657408e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.41225729 0.80281907]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60753817 0.60753818]\n",
      "Conjectured b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Norm of difference: \n",
      " 3.855224492198642e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Conjectured b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Norm of difference: \n",
      " 9.790219675713879e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Conjectured b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Norm of difference: \n",
      " 7.692236225895178e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60753817 0.60753818]\n",
      "Conjectured b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Norm of difference: \n",
      " 3.805629824696942e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60753817 0.60753818]\n",
      "Conjectured b_best: \n",
      " [0.60753818 0.60753818]\n",
      "Norm of difference: \n",
      " 3.855224492198642e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 3, lambda = 2\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.54074251 0.19911933 0.05077298]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.29575773 0.19911933 0.29575776]\n",
      "Conjectured b_best: \n",
      " [0.29575775 0.19911933 0.29575775]\n",
      "Norm of difference: \n",
      " 1.6522205645455014e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.29575774 0.19911933 0.29575775]\n",
      "Conjectured b_best: \n",
      " [0.29575775 0.19911933 0.29575775]\n",
      "Norm of difference: \n",
      " 2.265874613995459e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.29575775 0.19911933 0.29575775]\n",
      "Conjectured b_best: \n",
      " [0.29575775 0.19911933 0.29575775]\n",
      "Norm of difference: \n",
      " 8.952560433127798e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.29575774 0.19911933 0.29575776]\n",
      "Conjectured b_best: \n",
      " [0.29575775 0.19911933 0.29575775]\n",
      "Norm of difference: \n",
      " 1.2998049121992743e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.29575774 0.19911933 0.29575776]\n",
      "Conjectured b_best: \n",
      " [0.29575775 0.19911933 0.29575775]\n",
      "Norm of difference: \n",
      " 1.2998049121992743e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.41200154 0.60205422 0.73354117]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60205422 0.57277135 0.57277136]\n",
      "Conjectured b_best: \n",
      " [0.60205422 0.57277136 0.57277136]\n",
      "Norm of difference: \n",
      " 4.835325898335467e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60205422 0.57277135 0.57277136]\n",
      "Conjectured b_best: \n",
      " [0.60205422 0.57277136 0.57277136]\n",
      "Norm of difference: \n",
      " 1.541603131021054e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60205422 0.57277135 0.57277136]\n",
      "Conjectured b_best: \n",
      " [0.60205422 0.57277136 0.57277136]\n",
      "Norm of difference: \n",
      " 3.6501189180619034e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60205422 0.57277136 0.57277135]\n",
      "Conjectured b_best: \n",
      " [0.60205422 0.57277136 0.57277136]\n",
      "Norm of difference: \n",
      " 1.899723321792698e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.60205422 0.57277135 0.57277136]\n",
      "Conjectured b_best: \n",
      " [0.60205422 0.57277136 0.57277136]\n",
      "Norm of difference: \n",
      " 1.541603131021054e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.50680207 0.50761235 0.40820541]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Conjectured b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Norm of difference: \n",
      " 4.37554543965374e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Conjectured b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Norm of difference: \n",
      " 5.680705207055058e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Conjectured b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Norm of difference: \n",
      " 1.5295255873073098e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Conjectured b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Norm of difference: \n",
      " 1.8414044364479914e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Conjectured b_best: \n",
      " [0.50680207 0.45790888 0.45790888]\n",
      "Norm of difference: \n",
      " 1.5295255873073098e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.90681082 0.54386024 0.33470063]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Conjectured b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Norm of difference: \n",
      " 5.413074311948137e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62075573 0.54386024 0.62075572]\n",
      "Conjectured b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Norm of difference: \n",
      " 4.395238402721057e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Conjectured b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Norm of difference: \n",
      " 1.2294822530299838e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Conjectured b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Norm of difference: \n",
      " 2.0862446535523655e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62075573 0.54386024 0.62075572]\n",
      "Conjectured b_best: \n",
      " [0.62075572 0.54386024 0.62075572]\n",
      "Norm of difference: \n",
      " 4.395238402721057e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.15441415 0.50551384 0.4383264 ]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Conjectured b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Norm of difference: \n",
      " 1.953253924398099e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Conjectured b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Norm of difference: \n",
      " 1.4257047543291507e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.4424906541753444e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Conjectured b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Norm of difference: \n",
      " 6.732338220431503e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.3306690738754696e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4383264  0.329964   0.32996399]\n",
      "Conjectured b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Norm of difference: \n",
      " 3.3377383467791874e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4383264  0.329964   0.32996399]\n",
      "Conjectured b_best: \n",
      " [0.4383264  0.32996399 0.32996399]\n",
      "Norm of difference: \n",
      " 3.3377383467791874e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.20313556 0.79128966 0.91626196]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Conjectured b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Norm of difference: \n",
      " 7.518162430151281e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79128966 0.55969878 0.55969875]\n",
      "Conjectured b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Norm of difference: \n",
      " 2.045251965248099e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79128966 0.55969877 0.55969876]\n",
      "Conjectured b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Norm of difference: \n",
      " 5.430077078694051e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79128966 0.55969877 0.55969875]\n",
      "Conjectured b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Norm of difference: \n",
      " 1.6845570191056906e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.3306690738754696e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Conjectured b_best: \n",
      " [0.79128966 0.55969876 0.55969876]\n",
      "Norm of difference: \n",
      " 7.518162430151281e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.50060239 0.10145241 0.87235811]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Conjectured b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Norm of difference: \n",
      " 5.550502856256576e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50060239 0.48690528 0.48690524]\n",
      "Conjectured b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Norm of difference: \n",
      " 2.152366683435204e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Conjectured b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Norm of difference: \n",
      " 2.27343299983904e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50060239 0.48690524 0.48690528]\n",
      "Conjectured b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Norm of difference: \n",
      " 2.252438594882897e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Conjectured b_best: \n",
      " [0.50060239 0.48690526 0.48690526]\n",
      "Norm of difference: \n",
      " 5.550502856256576e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.9949172  0.85878038 0.19250189]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85878038 0.59370955 0.59370954]\n",
      "Conjectured b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Norm of difference: \n",
      " 5.532874564672079e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85878038 0.59370955 0.59370954]\n",
      "Conjectured b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Norm of difference: \n",
      " 3.362482532899175e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85878038 0.59370954 0.59370955]\n",
      "Conjectured b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Norm of difference: \n",
      " 3.48908866540688e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Conjectured b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Norm of difference: \n",
      " 5.982082884389827e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -6.661338147750939e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Conjectured b_best: \n",
      " [0.85878038 0.59370954 0.59370954]\n",
      "Norm of difference: \n",
      " 5.982082884389827e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -6.661338147750939e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.0235496  0.32451905 0.76591612]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Conjectured b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Norm of difference: \n",
      " 3.136667949512522e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39473288 0.32451905 0.39473284]\n",
      "Conjectured b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Norm of difference: \n",
      " 3.28280401463731e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.3306690738754696e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39473285 0.32451905 0.39473287]\n",
      "Conjectured b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Norm of difference: \n",
      " 1.2170210454464854e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Conjectured b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Norm of difference: \n",
      " 4.066280010212187e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39473285 0.32451905 0.39473287]\n",
      "Conjectured b_best: \n",
      " [0.39473286 0.32451905 0.39473286]\n",
      "Norm of difference: \n",
      " 1.2170210454464854e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.24545606 0.37636083 0.74590766]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Conjectured b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Norm of difference: \n",
      " 4.543265856182533e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Conjectured b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Norm of difference: \n",
      " 1.806350472807137e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Conjectured b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Norm of difference: \n",
      " 2.8024591285695676e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Conjectured b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Norm of difference: \n",
      " 2.5788268559433213e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Conjectured b_best: \n",
      " [0.49568186 0.37636083 0.49568186]\n",
      "Norm of difference: \n",
      " 4.543265856182533e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 4, lambda = 2\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.57166929 0.83195986 0.7567374  0.73105459]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.74389599 0.70181457 0.743896   0.70181457]\n",
      "Conjectured b_best: \n",
      " [0.74389599 0.70181457 0.74389599 0.70181457]\n",
      "Norm of difference: \n",
      " 1.3682451679932957e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.74389599 0.70181457 0.743896   0.70181457]\n",
      "Conjectured b_best: \n",
      " [0.74389599 0.70181457 0.74389599 0.70181457]\n",
      "Norm of difference: \n",
      " 1.5984804523436657e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.74389599 0.70181457 0.743896   0.70181457]\n",
      "Conjectured b_best: \n",
      " [0.74389599 0.70181457 0.74389599 0.70181457]\n",
      "Norm of difference: \n",
      " 7.000420504051413e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.74389599 0.70181457 0.743896   0.70181457]\n",
      "Conjectured b_best: \n",
      " [0.74389599 0.70181457 0.74389599 0.70181457]\n",
      "Norm of difference: \n",
      " 8.744985232766113e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.74389599 0.70181457 0.743896   0.70181457]\n",
      "Conjectured b_best: \n",
      " [0.74389599 0.70181457 0.74389599 0.70181457]\n",
      "Norm of difference: \n",
      " 8.744985232766113e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.77828186 0.62256615 0.46337383 0.12804862]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54296997 0.45316524 0.54297001 0.45316524]\n",
      "Conjectured b_best: \n",
      " [0.54296999 0.45316524 0.54296999 0.45316524]\n",
      "Norm of difference: \n",
      " 2.457692430332534e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54297    0.45316524 0.54296998 0.45316524]\n",
      "Conjectured b_best: \n",
      " [0.54296999 0.45316524 0.54296999 0.45316524]\n",
      "Norm of difference: \n",
      " 1.54953144234491e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54296999 0.45316521 0.54296999 0.45316526]\n",
      "Conjectured b_best: \n",
      " [0.54296999 0.45316524 0.54296999 0.45316524]\n",
      "Norm of difference: \n",
      " 3.800967384483198e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.552713678800501e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54297    0.45316524 0.54296997 0.45316524]\n",
      "Conjectured b_best: \n",
      " [0.54296999 0.45316524 0.54296999 0.45316524]\n",
      "Norm of difference: \n",
      " 2.0903502256679084e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54297    0.45316524 0.54296998 0.45316524]\n",
      "Conjectured b_best: \n",
      " [0.54296999 0.45316524 0.54296999 0.45316524]\n",
      "Norm of difference: \n",
      " 1.54953144234491e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.07488564 0.45296486 0.84894766 0.11535955]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46191667 0.28416221 0.46191663 0.2841622 ]\n",
      "Conjectured b_best: \n",
      " [0.46191665 0.2841622  0.46191665 0.2841622 ]\n",
      "Norm of difference: \n",
      " 2.886604029056362e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.773159728050814e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46191665 0.2841622  0.46191664 0.28416221]\n",
      "Conjectured b_best: \n",
      " [0.46191665 0.2841622  0.46191665 0.2841622 ]\n",
      "Norm of difference: \n",
      " 1.1418810072120879e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46191664 0.2841622  0.46191666 0.28416221]\n",
      "Conjectured b_best: \n",
      " [0.46191665 0.2841622  0.46191665 0.2841622 ]\n",
      "Norm of difference: \n",
      " 1.782421999608694e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46191666 0.28416221 0.46191663 0.2841622 ]\n",
      "Conjectured b_best: \n",
      " [0.46191665 0.2841622  0.46191665 0.2841622 ]\n",
      "Norm of difference: \n",
      " 2.062594220934201e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46191665 0.2841622  0.46191664 0.28416221]\n",
      "Conjectured b_best: \n",
      " [0.46191665 0.2841622  0.46191665 0.2841622 ]\n",
      "Norm of difference: \n",
      " 1.1418810072120879e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.0238022  0.09986587 0.21791997 0.2641513 ]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Conjectured b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Norm of difference: \n",
      " 2.211817056218056e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Conjectured b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Norm of difference: \n",
      " 5.828782945790164e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Conjectured b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Norm of difference: \n",
      " 2.618040640863965e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Conjectured b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Norm of difference: \n",
      " 3.318111573822764e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Conjectured b_best: \n",
      " [0.15889292 0.14397675 0.15889292 0.14397675]\n",
      "Norm of difference: \n",
      " 2.618040640863965e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.220446049250313e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.19992461 0.55456544 0.45234317 0.46356415]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4579532  0.37724503 0.45795412 0.37724502]\n",
      "Conjectured b_best: \n",
      " [0.45795366 0.37724502 0.45795366 0.37724502]\n",
      "Norm of difference: \n",
      " 6.549774664865508e-07\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.65085780787922e-13\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45795481 0.37724502 0.45795251 0.37724503]\n",
      "Conjectured b_best: \n",
      " [0.45795366 0.37724502 0.45795366 0.37724502]\n",
      "Norm of difference: \n",
      " 1.6251890240007455e-06\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.337952302397753e-12\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45795365 0.37724502 0.45795367 0.37724503]\n",
      "Conjectured b_best: \n",
      " [0.45795366 0.37724502 0.45795366 0.37724502]\n",
      "Norm of difference: \n",
      " 1.3788724044486237e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4579538  0.37724502 0.45795352 0.37724503]\n",
      "Conjectured b_best: \n",
      " [0.45795366 0.37724502 0.45795366 0.37724502]\n",
      "Norm of difference: \n",
      " 1.9457705830088144e-07\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.72715225139109e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.45795365 0.37724502 0.45795367 0.37724503]\n",
      "Conjectured b_best: \n",
      " [0.45795366 0.37724502 0.45795366 0.37724502]\n",
      "Norm of difference: \n",
      " 1.3788724044486237e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.1203775  0.92081911 0.21420818 0.67785342]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Conjectured b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Norm of difference: \n",
      " 5.2790434250313045e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Conjectured b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Norm of difference: \n",
      " 2.2831066805195977e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.2878587085651816e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Conjectured b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Norm of difference: \n",
      " 1.4215615871515364e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.52059829 0.44603079 0.52059831 0.44603081]\n",
      "Conjectured b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Norm of difference: \n",
      " 2.2227453064718634e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Conjectured b_best: \n",
      " [0.5205983 0.4460308 0.5205983 0.4460308]\n",
      "Norm of difference: \n",
      " 5.2790434250313045e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.39049916 0.98905897 0.2628976  0.3212071 ]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62597829 0.35585313 0.62597828 0.35585313]\n",
      "Conjectured b_best: \n",
      " [0.62597828 0.35585313 0.62597828 0.35585313]\n",
      "Norm of difference: \n",
      " 1.88559540786521e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62597829 0.35585316 0.62597828 0.35585309]\n",
      "Conjectured b_best: \n",
      " [0.62597828 0.35585313 0.62597828 0.35585313]\n",
      "Norm of difference: \n",
      " 4.8574802954730546e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.329070518200751e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.6259783  0.35585316 0.62597827 0.35585309]\n",
      "Conjectured b_best: \n",
      " [0.62597828 0.35585313 0.62597828 0.35585313]\n",
      "Norm of difference: \n",
      " 5.2757066269066714e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62597829 0.35585312 0.62597828 0.35585313]\n",
      "Conjectured b_best: \n",
      " [0.62597828 0.35585313 0.62597828 0.35585313]\n",
      "Norm of difference: \n",
      " 1.006034484759469e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.62597829 0.35585313 0.62597828 0.35585313]\n",
      "Conjectured b_best: \n",
      " [0.62597828 0.35585313 0.62597828 0.35585313]\n",
      "Norm of difference: \n",
      " 1.88559540786521e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.35034985 0.04681572 0.28821868 0.56300588]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.31928425 0.3049108  0.31928427 0.3049108 ]\n",
      "Conjectured b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.3049108 ]\n",
      "Norm of difference: \n",
      " 1.6144396291523488e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.31928424 0.3049108  0.31928428 0.3049108 ]\n",
      "Conjectured b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.3049108 ]\n",
      "Norm of difference: \n",
      " 3.128533812573352e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.30491081]\n",
      "Conjectured b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.3049108 ]\n",
      "Norm of difference: \n",
      " 3.973740754261826e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.31928425 0.3049108  0.31928427 0.3049108 ]\n",
      "Conjectured b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.3049108 ]\n",
      "Norm of difference: \n",
      " 1.6445310151740194e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.30491081]\n",
      "Conjectured b_best: \n",
      " [0.31928426 0.3049108  0.31928426 0.3049108 ]\n",
      "Norm of difference: \n",
      " 3.973740754261826e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.35208745 0.66159124 0.73195427 0.41517862]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54202086 0.53838492 0.54202086 0.53838493]\n",
      "Conjectured b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Norm of difference: \n",
      " 7.984983423791944e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54202087 0.53838493 0.54202085 0.53838492]\n",
      "Conjectured b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Norm of difference: \n",
      " 1.1977619416246654e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Conjectured b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Norm of difference: \n",
      " 1.870788129581413e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54202086 0.53838493 0.54202087 0.53838493]\n",
      "Conjectured b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Norm of difference: \n",
      " 5.306595484658645e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Conjectured b_best: \n",
      " [0.54202086 0.53838493 0.54202086 0.53838493]\n",
      "Norm of difference: \n",
      " 1.870788129581413e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.32128035 0.71820951 0.17629791 0.32768586]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44725283 0.32335282 0.4472546  0.32561339]\n",
      "Conjectured b_best: \n",
      " [0.44725371 0.3244831  0.44725371 0.3244831 ]\n",
      "Norm of difference: \n",
      " 0.0015984595514264388\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.828645160299573e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44725358 0.32430768 0.44725384 0.32465853]\n",
      "Conjectured b_best: \n",
      " [0.44725371 0.3244831  0.44725371 0.3244831 ]\n",
      "Norm of difference: \n",
      " 0.0002480900825952689\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4042170359829242e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4472539  0.3237367  0.44725352 0.32522951]\n",
      "Conjectured b_best: \n",
      " [0.44725371 0.3244831  0.44725371 0.3244831 ]\n",
      "Norm of difference: \n",
      " 0.0010555816183962956\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.541827243351946e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44725749 0.32262529 0.44724995 0.3263409 ]\n",
      "Conjectured b_best: \n",
      " [0.44725371 0.3244831  0.44725371 0.3244831 ]\n",
      "Norm of difference: \n",
      " 0.0026273439072843183\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.575703268574813e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44725358 0.32430768 0.44725384 0.32465853]\n",
      "Conjectured b_best: \n",
      " [0.44725371 0.3244831  0.44725371 0.3244831 ]\n",
      "Norm of difference: \n",
      " 0.0002480900825952689\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4042170359829242e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 4, lambda = 3\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.27439654 0.44049534 0.7155156  0.03696871]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624215]\n",
      "Conjectured b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624216]\n",
      "Norm of difference: \n",
      " 4.7471081731717245e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624215]\n",
      "Conjectured b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624216]\n",
      "Norm of difference: \n",
      " 1.551972374889093e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624215]\n",
      "Conjectured b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624216]\n",
      "Norm of difference: \n",
      " 6.371445088921292e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44049534 0.37624215 0.27439654 0.37624216]\n",
      "Conjectured b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624216]\n",
      "Norm of difference: \n",
      " 4.565814346497224e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624215]\n",
      "Conjectured b_best: \n",
      " [0.44049534 0.37624216 0.27439654 0.37624216]\n",
      "Norm of difference: \n",
      " 1.551972374889093e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.19695701 0.15467245 0.57123657 0.63162861]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315054]\n",
      "Conjectured b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Norm of difference: \n",
      " 3.410154033401993e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.552713678800501e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Conjectured b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Norm of difference: \n",
      " 1.9555949049581743e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315054]\n",
      "Conjectured b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Norm of difference: \n",
      " 2.620530631270789e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315054]\n",
      "Conjectured b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Norm of difference: \n",
      " 5.212870527212417e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Conjectured b_best: \n",
      " [0.57123657 0.39315053 0.19695701 0.39315053]\n",
      "Norm of difference: \n",
      " 1.9555949049581743e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.79603133 0.46385504 0.80165145 0.18074733]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Conjectured b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Norm of difference: \n",
      " 1.3550587581560648e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119938]\n",
      "Conjectured b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Norm of difference: \n",
      " 3.1244031330197235e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79603133 0.49119938 0.46385504 0.49119939]\n",
      "Conjectured b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Norm of difference: \n",
      " 9.390236777005525e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Conjectured b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Norm of difference: \n",
      " 1.1167434518846496e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Conjectured b_best: \n",
      " [0.79603133 0.49119939 0.46385504 0.49119939]\n",
      "Norm of difference: \n",
      " 1.1167434518846496e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.94656111 0.11803398 0.12874317 0.55719218]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Conjectured b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Norm of difference: \n",
      " 2.0499190732202783e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.021405182655144e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Conjectured b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Norm of difference: \n",
      " 6.071133826035747e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.217248937900877e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Conjectured b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Norm of difference: \n",
      " 3.3727681304414434e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4654943925052066e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Conjectured b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Norm of difference: \n",
      " 3.0737169372605065e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Conjectured b_best: \n",
      " [0.55719218 0.53229755 0.12874317 0.53229755]\n",
      "Norm of difference: \n",
      " 3.0737169372605065e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.5767778  0.52406921 0.68752871 0.82935604]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Conjectured b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Norm of difference: \n",
      " 1.8532475644557546e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Conjectured b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Norm of difference: \n",
      " 2.1782285224079227e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Conjectured b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Norm of difference: \n",
      " 3.2625186713897925e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Conjectured b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Norm of difference: \n",
      " 1.1374461022597882e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Conjectured b_best: \n",
      " [0.68752871 0.67671262 0.5767778  0.67671262]\n",
      "Norm of difference: \n",
      " 2.1782285224079227e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.53712033 0.03992063 0.3045137  0.55851877]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.29921969]\n",
      "Conjectured b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Norm of difference: \n",
      " 3.6045887881617884e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.53712033 0.3045137  0.29921969 0.2992197 ]\n",
      "Conjectured b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Norm of difference: \n",
      " 7.843784062157734e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.53712033 0.3045137  0.29921969 0.29921971]\n",
      "Conjectured b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Norm of difference: \n",
      " 1.6253220427872196e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.993605777301127e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Conjectured b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Norm of difference: \n",
      " 1.1790024459045055e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Conjectured b_best: \n",
      " [0.53712033 0.3045137  0.2992197  0.2992197 ]\n",
      "Norm of difference: \n",
      " 1.1790024459045055e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.29248618 0.63590455 0.56397839 0.07741039]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Conjectured b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Norm of difference: \n",
      " 2.4466625176273436e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Conjectured b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Norm of difference: \n",
      " 1.0885211576989342e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Conjectured b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Norm of difference: \n",
      " 1.2549756872722604e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Conjectured b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Norm of difference: \n",
      " 1.0921633011729896e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Conjectured b_best: \n",
      " [0.56397839 0.35665747 0.29248618 0.35665747]\n",
      "Norm of difference: \n",
      " 2.4466625176273436e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.98947514 0.75589156 0.56005141 0.85022452]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Conjectured b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Norm of difference: \n",
      " 6.544539461030014e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -3.552713678800501e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Conjectured b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Norm of difference: \n",
      " 1.5106657152157557e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Conjectured b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Norm of difference: \n",
      " 2.1932050826773513e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -5.773159728050814e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Conjectured b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Norm of difference: \n",
      " 1.1595840919109478e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Conjectured b_best: \n",
      " [0.85022452 0.77476328 0.75589156 0.77476328]\n",
      "Norm of difference: \n",
      " 2.1932050826773513e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -5.773159728050814e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.45066951 0.30797527 0.11663765 0.88932937]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Conjectured b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Norm of difference: \n",
      " 1.0264643523307443e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50298352 0.45066951 0.30797527 0.50298351]\n",
      "Conjectured b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Norm of difference: \n",
      " 5.17199862612174e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Conjectured b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Norm of difference: \n",
      " 5.099184567793365e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50298352 0.45066951 0.30797527 0.50298351]\n",
      "Conjectured b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Norm of difference: \n",
      " 5.941243523466019e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Conjectured b_best: \n",
      " [0.50298351 0.45066951 0.30797527 0.50298351]\n",
      "Norm of difference: \n",
      " 1.0264643523307443e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.85374244 0.40728798 0.14320572 0.13172237]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4927324  0.40728798 0.14320572 0.49273242]\n",
      "Conjectured b_best: \n",
      " [0.49273241 0.40728798 0.14320572 0.49273241]\n",
      "Norm of difference: \n",
      " 1.1556282282573768e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.9968028886505635e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49273242 0.40728798 0.14320572 0.49273239]\n",
      "Conjectured b_best: \n",
      " [0.49273241 0.40728798 0.14320572 0.49273241]\n",
      "Norm of difference: \n",
      " 2.091685212332006e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 9.769962616701378e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49273241 0.40728798 0.14320572 0.4927324 ]\n",
      "Conjectured b_best: \n",
      " [0.49273241 0.40728798 0.14320572 0.49273241]\n",
      "Norm of difference: \n",
      " 6.089816466290298e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4927324  0.40728798 0.14320572 0.49273241]\n",
      "Conjectured b_best: \n",
      " [0.49273241 0.40728798 0.14320572 0.49273241]\n",
      "Norm of difference: \n",
      " 5.531548229067806e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4927324  0.40728798 0.14320572 0.49273241]\n",
      "Conjectured b_best: \n",
      " [0.49273241 0.40728798 0.14320572 0.49273241]\n",
      "Norm of difference: \n",
      " 5.531548229067806e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 5, lambda = 3\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.2176383  0.83655366 0.8306726  0.89681421 0.21631172]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.83067264 0.55681502 0.52684683 0.55681248 0.52684353]\n",
      "Conjectured b_best: \n",
      " [0.8306726  0.55656297 0.52709598 0.55656297 0.52709598]\n",
      "Norm of difference: \n",
      " 0.0005015919391547589\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.01148147096886e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.8306726  0.5565641  0.52709435 0.55656819 0.52709126]\n",
      "Conjectured b_best: \n",
      " [0.8306726  0.55656297 0.52709598 0.55656297 0.52709598]\n",
      "Norm of difference: \n",
      " 7.310993483530402e-06\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.542454052433811e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.83067262 0.55707077 0.52661603 0.55703003 0.52660104]\n",
      "Conjectured b_best: \n",
      " [0.8306726  0.55656297 0.52709598 0.55656297 0.52709598]\n",
      "Norm of difference: \n",
      " 0.0009753605719315291\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.00011769533961603074\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.83067263 0.55697611 0.52671608 0.55693633 0.52668935]\n",
      "Conjectured b_best: \n",
      " [0.8306726  0.55656297 0.52709598 0.55656297 0.52709598]\n",
      "Norm of difference: \n",
      " 0.0007872524981647649\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 9.466741743713314e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.8306726  0.5565641  0.52709435 0.55656819 0.52709126]\n",
      "Conjectured b_best: \n",
      " [0.8306726  0.55656297 0.52709598 0.55656297 0.52709598]\n",
      "Norm of difference: \n",
      " 7.310993483530402e-06\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.542454052433811e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.47712994 0.9789114  0.39902159 0.47038567 0.93076552]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.7005757  0.68896663 0.47712993 0.70057549 0.68896636]\n",
      "Conjectured b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Norm of difference: \n",
      " 2.4252551031148817e-07\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.93249191385803e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Conjectured b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Norm of difference: \n",
      " 3.500260838499201e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.70057561 0.68896648 0.47712994 0.70057557 0.68896651]\n",
      "Conjectured b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Norm of difference: \n",
      " 3.4232828159808175e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6773249456036865e-12\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.7005756  0.68896649 0.47712994 0.70057559 0.6889665 ]\n",
      "Conjectured b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Norm of difference: \n",
      " 1.1770792196074115e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.398081733190338e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Conjectured b_best: \n",
      " [0.70057559 0.68896649 0.47712994 0.70057559 0.68896649]\n",
      "Norm of difference: \n",
      " 3.500260838499201e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.01447991 0.43433388 0.00315888 0.90462411 0.91434125]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46015357 0.45811762 0.43433388 0.46017596 0.45815701]\n",
      "Conjectured b_best: \n",
      " [0.45955201 0.45875007 0.43433388 0.45955201 0.45875007]\n",
      "Norm of difference: \n",
      " 0.001225929565174231\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.984390785280681e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46269242 0.45557879 0.43433369 0.46273313 0.4556    ]\n",
      "Conjectured b_best: \n",
      " [0.45955201 0.45875007 0.43433388 0.45955201 0.45875007]\n",
      "Norm of difference: \n",
      " 0.0063215257483752\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.00010089428947024359\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46356168 0.45474815 0.43433383 0.4635428  0.45475158]\n",
      "Conjectured b_best: \n",
      " [0.45955201 0.45875007 0.43433388 0.45955201 0.45875007]\n",
      "Norm of difference: \n",
      " 0.00800044440090759\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0001547179008141164\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4600162  0.45828845 0.43433388 0.46001355 0.45828595]\n",
      "Conjectured b_best: \n",
      " [0.45955201 0.45875007 0.43433388 0.45955201 0.45875007]\n",
      "Norm of difference: \n",
      " 0.0009257393827779527\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.71517840772151e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4600162  0.45828845 0.43433388 0.46001355 0.45828595]\n",
      "Conjectured b_best: \n",
      " [0.45955201 0.45875007 0.43433388 0.45955201 0.45875007]\n",
      "Norm of difference: \n",
      " 0.0009257393827779527\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.71517840772151e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.02212841 0.48954589 0.39477084 0.9626704  0.13533846]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244218]\n",
      "Conjectured b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244217]\n",
      "Norm of difference: \n",
      " 3.277809618474031e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4923994  0.39477084 0.31244218 0.49239941 0.31244217]\n",
      "Conjectured b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244217]\n",
      "Norm of difference: \n",
      " 1.1876871325654436e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4923994  0.39477084 0.31244217 0.49239941 0.31244217]\n",
      "Conjectured b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244217]\n",
      "Norm of difference: \n",
      " 3.294902863549931e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.4923994  0.31244218]\n",
      "Conjectured b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244217]\n",
      "Norm of difference: \n",
      " 3.5751817164017232e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244218]\n",
      "Conjectured b_best: \n",
      " [0.49239941 0.39477084 0.31244217 0.49239941 0.31244217]\n",
      "Norm of difference: \n",
      " 3.277809618474031e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.02666984 0.07681016 0.50876872 0.15699166 0.74181509]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.38424246 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Conjectured b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Norm of difference: \n",
      " 2.838486340330558e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.1086244689504383e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Conjectured b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Norm of difference: \n",
      " 6.0127786393085925e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Conjectured b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Norm of difference: \n",
      " 3.177699139120393e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.38424246 0.29278944 0.15699166 0.38424248 0.29278944]\n",
      "Conjectured b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Norm of difference: \n",
      " 1.1865551564158978e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.549516567451064e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Conjectured b_best: \n",
      " [0.38424247 0.29278944 0.15699166 0.38424247 0.29278944]\n",
      "Norm of difference: \n",
      " 6.0127786393085925e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.35106668 0.85071066 0.86811862 0.29328189 0.68730357]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Conjectured b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Norm of difference: \n",
      " 1.372538202668045e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.2434497875801753e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Conjectured b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Norm of difference: \n",
      " 6.037928494119427e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.10702591327572e-13\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Conjectured b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Norm of difference: \n",
      " 4.84603471581971e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.284661597215745e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68730357 0.60088859 0.58070007 0.60088876 0.58070043]\n",
      "Conjectured b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Norm of difference: \n",
      " 2.7757052372688477e-07\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.922124764381806e-10\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Conjectured b_best: \n",
      " [0.68730357 0.60088867 0.58070025 0.60088867 0.58070025]\n",
      "Norm of difference: \n",
      " 1.372538202668045e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.2434497875801753e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.40371116 0.43894605 0.65035267 0.40332722 0.17088356]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.42126095 0.41061833 0.40352126 0.42120221 0.41061791]\n",
      "Conjectured b_best: \n",
      " [0.42113663 0.41061812 0.40371116 0.42113663 0.41061812]\n",
      "Norm of difference: \n",
      " 0.0002362488910534229\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3771561859776682e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.42123087 0.41061726 0.40344834 0.42130523 0.41061896]\n",
      "Conjectured b_best: \n",
      " [0.42113663 0.41061812 0.40371116 0.42113663 0.41061812]\n",
      "Norm of difference: \n",
      " 0.0003261537419494321\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9121123192800127e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.42114327 0.41061837 0.40367136 0.4211698  0.41061786]\n",
      "Conjectured b_best: \n",
      " [0.42113663 0.41061812 0.40371116 0.42113663 0.41061812]\n",
      "Norm of difference: \n",
      " 5.2229312276905124e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.867791376370832e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.42116348 0.41061798 0.40357479 0.42124616 0.41061825]\n",
      "Conjectured b_best: \n",
      " [0.42113663 0.41061812 0.40371116 0.42113663 0.41061812]\n",
      "Norm of difference: \n",
      " 0.000176952539444375\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 9.871910559233754e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.42114327 0.41061837 0.40367136 0.4211698  0.41061786]\n",
      "Conjectured b_best: \n",
      " [0.42113663 0.41061812 0.40371116 0.42113663 0.41061812]\n",
      "Norm of difference: \n",
      " 5.2229312276905124e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.867791376370832e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.5497431  0.49486559 0.95415708 0.51297451 0.20320371]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57868027 0.5223083  0.51297127 0.57868052 0.52230363]\n",
      "Conjectured b_best: \n",
      " [0.5786804  0.52230434 0.51297451 0.5786804  0.52230434]\n",
      "Norm of difference: \n",
      " 5.171530340252993e-06\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.2144914718348332e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57868066 0.52231214 0.51297295 0.57868014 0.5222981 ]\n",
      "Conjectured b_best: \n",
      " [0.5786804  0.52230434 0.51297451 0.5786804  0.52230434]\n",
      "Norm of difference: \n",
      " 1.0118221483090986e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.863353935708915e-08\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57868402 0.52241479 0.5128597  0.57867679 0.5223087 ]\n",
      "Conjectured b_best: \n",
      " [0.5786804  0.52230434 0.51297451 0.5786804  0.52230434]\n",
      "Norm of difference: \n",
      " 0.00015944824183442092\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.342773085141971e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57868039 0.52230426 0.51297451 0.5786804  0.52230442]\n",
      "Conjectured b_best: \n",
      " [0.5786804  0.52230434 0.51297451 0.5786804  0.52230434]\n",
      "Norm of difference: \n",
      " 1.1564132753496579e-07\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3065104553788842e-12\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.57868039 0.52230426 0.51297451 0.5786804  0.52230442]\n",
      "Conjectured b_best: \n",
      " [0.5786804  0.52230434 0.51297451 0.5786804  0.52230434]\n",
      "Norm of difference: \n",
      " 1.1564132753496579e-07\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3065104553788842e-12\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.13010394 0.67520259 0.43213249 0.39645286 0.36451756]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40265326 0.39832502 0.39645286 0.40265326 0.39832503]\n",
      "Conjectured b_best: \n",
      " [0.40265326 0.39832502 0.39645286 0.40265326 0.39832502]\n",
      "Norm of difference: \n",
      " 3.702032085193712e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4654943925052066e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40265326 0.39832503 0.39645286 0.40265326 0.39832502]\n",
      "Conjectured b_best: \n",
      " [0.40265326 0.39832502 0.39645286 0.40265326 0.39832502]\n",
      "Norm of difference: \n",
      " 7.491435527911705e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.217248937900877e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40265326 0.39832503 0.39645286 0.40265326 0.39832502]\n",
      "Conjectured b_best: \n",
      " [0.40265326 0.39832502 0.39645286 0.40265326 0.39832502]\n",
      "Norm of difference: \n",
      " 3.957285178840932e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40265326 0.39832503 0.39645286 0.40265326 0.39832502]\n",
      "Conjectured b_best: \n",
      " [0.40265326 0.39832502 0.39645286 0.40265326 0.39832502]\n",
      "Norm of difference: \n",
      " 3.931059019552083e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40265326 0.39832503 0.39645286 0.40265326 0.39832502]\n",
      "Conjectured b_best: \n",
      " [0.40265326 0.39832502 0.39645286 0.40265326 0.39832502]\n",
      "Norm of difference: \n",
      " 3.957285178840932e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.00307448 0.55902119 0.82623378 0.27162349 0.80788072]\n",
      "  lambda = 3\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Conjectured b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Norm of difference: \n",
      " 3.959124905934435e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9984014443252818e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55902119 0.53975211 0.41465412 0.5397521  0.41465413]\n",
      "Conjectured b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Norm of difference: \n",
      " 1.4061537489738027e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.927791673660977e-12\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Conjectured b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Norm of difference: \n",
      " 3.0601680681952384e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55902119 0.5397521  0.41465413 0.53975211 0.41465413]\n",
      "Conjectured b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Norm of difference: \n",
      " 3.99673075227477e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.0658141036401503e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Conjectured b_best: \n",
      " [0.55902119 0.53975211 0.41465413 0.53975211 0.41465413]\n",
      "Norm of difference: \n",
      " 3.0601680681952384e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 5, lambda = 4\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.76270725 0.76945028 0.42868006 0.19726947 0.59546854]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Conjectured b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Norm of difference: \n",
      " 1.0426913365085765e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.76270725 0.59546854 0.48335988 0.42868006 0.48335987]\n",
      "Conjectured b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Norm of difference: \n",
      " 4.658824190107415e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Conjectured b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Norm of difference: \n",
      " 1.2143184178212429e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.76270725 0.59546854 0.48335988 0.42868006 0.48335987]\n",
      "Conjectured b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Norm of difference: \n",
      " 2.129613354923972e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Conjectured b_best: \n",
      " [0.76270725 0.59546854 0.48335987 0.42868006 0.48335987]\n",
      "Norm of difference: \n",
      " 1.0426913365085765e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.26397174 0.13153856 0.8309782  0.02647089 0.5407854 ]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Conjectured b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Norm of difference: \n",
      " 1.9992603538362902e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Conjectured b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Norm of difference: \n",
      " 8.387725558964938e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5407854  0.42872454 0.26397174 0.13153856 0.42872455]\n",
      "Conjectured b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Norm of difference: \n",
      " 1.993405361236057e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872454]\n",
      "Conjectured b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Norm of difference: \n",
      " 1.402558176533035e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.1086244689504383e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Conjectured b_best: \n",
      " [0.5407854  0.42872455 0.26397174 0.13153856 0.42872455]\n",
      "Norm of difference: \n",
      " 1.9992603538362902e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.58723298 0.58357248 0.54115021 0.12599527 0.16505238]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661412]\n",
      "Conjectured b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661412]\n",
      "Norm of difference: \n",
      " 4.121382798933585e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661413]\n",
      "Conjectured b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661412]\n",
      "Norm of difference: \n",
      " 2.5620580329714646e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661413]\n",
      "Conjectured b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661412]\n",
      "Norm of difference: \n",
      " 2.4006816141808963e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.58357248 0.54115021 0.35661413 0.16505238 0.35661412]\n",
      "Conjectured b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661412]\n",
      "Norm of difference: \n",
      " 2.0257185705771346e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.58357248 0.54115021 0.35661413 0.16505238 0.35661412]\n",
      "Conjectured b_best: \n",
      " [0.58357248 0.54115021 0.35661412 0.16505238 0.35661412]\n",
      "Norm of difference: \n",
      " 2.0257185705771346e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.66630618 0.13835822 0.7058195  0.05793011 0.53339049]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.38187481]\n",
      "Conjectured b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Norm of difference: \n",
      " 2.7468353060301795e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.66630618 0.53339049 0.38187481 0.13835822 0.3818748 ]\n",
      "Conjectured b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Norm of difference: \n",
      " 5.254166471023731e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Conjectured b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Norm of difference: \n",
      " 3.099369186480513e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Conjectured b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Norm of difference: \n",
      " 1.1397743531156149e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Conjectured b_best: \n",
      " [0.66630618 0.53339049 0.3818748  0.13835822 0.3818748 ]\n",
      "Norm of difference: \n",
      " 3.099369186480513e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.04500664 0.75100626 0.40405075 0.49534603 0.42302498]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Conjectured b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Norm of difference: \n",
      " 4.94403667447562e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Conjectured b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Norm of difference: \n",
      " 1.963051391775843e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Conjectured b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Norm of difference: \n",
      " 4.566188814647276e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Conjectured b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Norm of difference: \n",
      " 3.9562946483445635e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Conjectured b_best: \n",
      " [0.49534603 0.42302498 0.40405075 0.39800645 0.39800645]\n",
      "Norm of difference: \n",
      " 1.963051391775843e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.71122464 0.30860022 0.50474343 0.37008513 0.38194699]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Conjectured b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Norm of difference: \n",
      " 4.116242314885829e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Conjectured b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Norm of difference: \n",
      " 7.477823614719757e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Conjectured b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Norm of difference: \n",
      " 1.0192420055305284e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Conjectured b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Norm of difference: \n",
      " 3.835900737618199e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Conjectured b_best: \n",
      " [0.50991243 0.50474343 0.38194699 0.37008513 0.50991243]\n",
      "Norm of difference: \n",
      " 3.835900737618199e-11\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.88668944 0.93721439 0.50443934 0.30879347 0.11818438]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769939]\n",
      "Conjectured b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Norm of difference: \n",
      " 7.659522014940315e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -3.1086244689504383e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Conjectured b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Norm of difference: \n",
      " 1.4007324730135677e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.1086244689504383e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769939]\n",
      "Conjectured b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Norm of difference: \n",
      " 4.362163463023991e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.552713678800501e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Conjectured b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Norm of difference: \n",
      " 6.113290800917482e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769939]\n",
      "Conjectured b_best: \n",
      " [0.88668944 0.52769938 0.50443934 0.30879347 0.52769938]\n",
      "Norm of difference: \n",
      " 7.659522014940315e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -3.1086244689504383e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.20198678 0.20592784 0.63845428 0.05935601 0.99168844]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Conjectured b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Norm of difference: \n",
      " 1.102886012106078e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Conjectured b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Norm of difference: \n",
      " 1.9363483975493556e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552223]\n",
      "Conjectured b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Norm of difference: \n",
      " 3.6484913602196797e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.63845428 0.52552223 0.20592784 0.20198678 0.52552222]\n",
      "Conjectured b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Norm of difference: \n",
      " 4.94653413410111e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -4.440892098500626e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Conjectured b_best: \n",
      " [0.63845428 0.52552222 0.20592784 0.20198678 0.52552222]\n",
      "Norm of difference: \n",
      " 1.9363483975493556e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.97976893 0.96470603 0.0728035  0.89272229 0.2905868 ]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.96470603 0.89272229 0.52628623 0.2905868  0.5262862 ]\n",
      "Conjectured b_best: \n",
      " [0.96470603 0.89272229 0.52628621 0.2905868  0.52628621]\n",
      "Norm of difference: \n",
      " 2.5720444882972235e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4210854715202004e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.96470603 0.89272229 0.52628622 0.2905868  0.52628621]\n",
      "Conjectured b_best: \n",
      " [0.96470603 0.89272229 0.52628621 0.2905868  0.52628621]\n",
      "Norm of difference: \n",
      " 1.7868861936064312e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.9968028886505635e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.96470603 0.89272229 0.52628622 0.2905868  0.52628621]\n",
      "Conjectured b_best: \n",
      " [0.96470603 0.89272229 0.52628621 0.2905868  0.52628621]\n",
      "Norm of difference: \n",
      " 1.925476841851217e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.96470603 0.89272229 0.52628622 0.2905868  0.52628621]\n",
      "Conjectured b_best: \n",
      " [0.96470603 0.89272229 0.52628621 0.2905868  0.52628621]\n",
      "Norm of difference: \n",
      " 4.743568674489938e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.1086244689504383e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.96470603 0.89272229 0.52628622 0.2905868  0.52628621]\n",
      "Conjectured b_best: \n",
      " [0.96470603 0.89272229 0.52628621 0.2905868  0.52628621]\n",
      "Norm of difference: \n",
      " 1.925476841851217e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.55885417 0.63964575 0.13542698 0.02450853 0.36198305]\n",
      "  lambda = 4\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Conjectured b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Norm of difference: \n",
      " 5.568847502569098e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Conjectured b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Norm of difference: \n",
      " 5.971876891358404e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55885417 0.36198305 0.33207715 0.13542698 0.33207714]\n",
      "Conjectured b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Norm of difference: \n",
      " 3.966038124477133e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Conjectured b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Norm of difference: \n",
      " 1.27468780604718e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 8.881784197001252e-16\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Conjectured b_best: \n",
      " [0.55885417 0.36198305 0.33207714 0.13542698 0.33207714]\n",
      "Norm of difference: \n",
      " 5.568847502569098e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " -2.6645352591003757e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n"
     ]
    }
   ],
   "source": [
    "# now for a rough way how one can test conjecture across various d and lamb values. should parallelize or use cluster instead!\n",
    "d_max = 5\n",
    "conjecture_holds = True\n",
    "# TODO: make N_n, N_init, (N_steps, learning_rate) d-dependent, i.e. upgrade them to lists of length d_max - 1\n",
    "N_n = 10\n",
    "N_init = 4\n",
    "N_steps = 1000\n",
    "learning_rate = 0.01\n",
    "eps = 1e-13\n",
    "for d in range(1, d_max + 1):\n",
    "    for lamb in range(int(np.ceil(d/2)), d):\n",
    "        print(F\"\\n{'='*40}\\n{'='*40}\\nCASE d = {d}, lambda = {lamb}\\n{'='*40}\\n{'='*40}\\n\")\n",
    "        for _ in range(N_n):\n",
    "            n = np.random.rand(d)\n",
    "            b_best_conj = nK(n, lamb)\n",
    "            U_best, b_best_num, H_best, conjecture_holds = get_b_best(n, lamb, N_init=N_init, N_steps=N_steps,learning_rate=learning_rate, eps=eps)\n",
    "            if conjecture_holds == False:\n",
    "                break\n",
    "        if conjecture_holds == False:\n",
    "            break\n",
    "    if conjecture_holds == False:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f02b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conjecture_holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b8c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaustav_conj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
