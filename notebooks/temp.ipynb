{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8655ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCalc is adding 2 + 3\n",
      "The computed sum is 5\n",
      "\n",
      "Quack! My name is Donald\n",
      "Quack!\n"
     ]
    }
   ],
   "source": [
    "# examples with classes\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store data in this instance\n",
    "    \n",
    "    def add(self, a, b):\n",
    "        print(f\"{self.name} is adding {a} + {b}\")  # Access self.name\n",
    "        return a + b\n",
    "\n",
    "# Usage:\n",
    "calc = Calculator(\"MyCalc\")\n",
    "summy = calc.add(2, 3)  # Python automatically passes calc as self\n",
    "print(f\"The computed sum is {summy}\\n\")\n",
    "\n",
    "class Duck:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store the duck's name\n",
    "    \n",
    "    def introduce(self):\n",
    "        print(f\"Quack! My name is {self.name}\")\n",
    "    \n",
    "    def quack(self):\n",
    "        print(\"Quack!\")\n",
    "\n",
    "# Usage examples:\n",
    "donald = Duck(\"Donald\")\n",
    "donald.introduce()\n",
    "donald.quack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9597f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1.009999999975, -1.9900000000026317, 4.99000000001, 3.1300000000159236], cost function = 52.8596\n",
      "x = [0.005897630008671893, -0.2904937204018134, 0.13457109604084116, 0.001668087453219903], cost function = 0.026028579594744642\n",
      "x = [0.00037449115354536817, -0.16550934894657934, 3.226168500859898e-05, 3.5483309608153823e-12], cost function = 0.0007721943108706685\n",
      "x = [5.4498576897916806e-05, -0.11256823123990456, 2.5927969375248464e-12, 7.114101782273717e-33], cost function = 0.00016282688486587434\n",
      "x = [1.097225479559197e-05, -0.08172503814583272, -3.4529096537761817e-28, 1.122277847165345e-55], cost function = 4.496092716242442e-05\n",
      "x = [2.5850164314183053e-06, -0.06122064525186035, 4.1293257701407547e-51, 2.185807862584084e-78], cost function = 1.4115670458544878e-05\n",
      "x = [6.622443599310257e-07, -0.04663116760110854, 5.564185457147956e-74, 1.3905268387082256e-101], cost function = 4.743645877179094e-06\n",
      "x = [1.777910982729993e-07, -0.03585051583371858, -6.917299300598905e-97, -5.502812919536767e-124], cost function = 1.655735660473815e-06\n",
      "x = [4.90287009399437e-08, -0.027709462286658784, -1.278435433395997e-119, -6.441845560901762e-147], cost function = 5.90595590587046e-07\n",
      "x = [1.3732527485209696e-08, -0.021483398961054796, 1.345962948601753e-142, 8.115096883895434e-170], cost function = 2.133303908361589e-07\n"
     ]
    }
   ],
   "source": [
    "# gradient descent basic example.\n",
    "# Not clear how \"non-analytical\" functions can get until torch stops working...\n",
    "# copilot claims torch ONLY uses analytical stuff, even for eigenvalues...??\n",
    "# general goot practice: prefer torch functions\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#possible cost functions\n",
    "def cost_function_1(x):\n",
    "    return x[0]**2 + x[1]**4 + (x[0] + x[1])**6 + x[2]**2 + x[3]**2\n",
    "\n",
    "def cost_function_2(x): # with eigenvalues involved\n",
    "    # Reshape x into a matrix\n",
    "    A = x.view(2, 2)  # If x has 4 elements\n",
    "    eigenvals = torch.linalg.eigvals(A)\n",
    "    eigenvals_real = torch.real(eigenvals)\n",
    "    return torch.sum(eigenvals_real**2)\n",
    "\n",
    "def cost_function_3(x):\n",
    "    return max(x)\n",
    "\n",
    "d = 2\n",
    "cost_function = cost_function_1\n",
    "# define variable to be optimized\n",
    "x = torch.tensor([1.0, -2.0, 5., 3.14], dtype=torch.double, requires_grad=True)\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 10000\n",
    "optimizer = torch.optim.Adam([x], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(x)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"x = {x.tolist()}, cost function = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 1000\n",
      "Step # 2000\n",
      "Step # 3000\n",
      "\n",
      " The optimized unitary is \n",
      " tensor([[ 0.6920+0.1455j,  0.5108-0.4890j],\n",
      "        [-0.3043-0.6383j,  0.6986+0.1092j]], dtype=torch.complex128) \n",
      " \n",
      "Absolute values of its entries: \n",
      " tensor([[0.7071, 0.7071],\n",
      "        [0.7071, 0.7071]], dtype=torch.float64) \n",
      " \n",
      "Compare to Hadamard: \n",
      " tensor([[ 0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]], dtype=torch.float64) \n",
      " \n",
      "The optimized block spectrum is \n",
      " tensor([0.3000, 0.3000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function\n",
    "\n",
    "\n",
    "n = [0.2, 0.4]\n",
    "lamb = 1\n",
    "# optimal rotation is e.g. a hadamard, yielding maximally mixed block spectrum [0.3, 0.3]\n",
    "# so optimal M should be s.t. M_to_A(M) = log(hadamard). here is one possibility:\n",
    "# M_optimal = torch.tensor([[ 0.4601,  0.0000], [-1.1107,  2.6815]], dtype=torch.double, requires_grad=True)\n",
    "\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized: here are some choices\n",
    "# M = torch.tensor([[0., 0.1], [0., 0.]], dtype=torch.double, requires_grad=True) # start with U0 close to identity\n",
    "# M = M_optimal\n",
    "M = torch.rand(2, 2) - 0.5 * torch.ones(2,2)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 3001\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized unitary and block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "H = torch.tensor([[1., 1.], [1., -1.]]/np.sqrt(2), dtype=torch.double)\n",
    "print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"Absolute values of its entries: \\n {torch.abs(U_best).data} \\n \")\n",
    "print(f\"Compare to Hadamard: \\n {H.data} \\n \")\n",
    "print(f\"The optimized block spectrum is \\n {b_best.data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4883994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 100\n",
      "Step # 200\n",
      "Step # 300\n",
      "Step # 400\n",
      "Step # 500\n",
      "Step # 600\n",
      "Step # 700\n",
      "Step # 800\n",
      "Step # 900\n",
      "Step # 1000\n",
      "The numerically optimized block spectrum is \n",
      " [0.60126569 0.52562717 0.49598848 0.47878389 0.3906772  0.52562717\n",
      " 0.49598848 0.47878389]\n",
      "Compare with the conjectured value: \n",
      " [0.60127 0.52563 0.49599 0.47878 0.39068 0.52563 0.49599 0.47878]\n",
      "Norm of difference: \n",
      " 1.877551628332819e-09\n"
     ]
    }
   ],
   "source": [
    "d = 8\n",
    "n = np.random.rand(d)\n",
    "lamb = 5\n",
    "learning_rate = 0.01\n",
    "n_steps = 1001\n",
    "\n",
    "# get conjectured bbest\n",
    "n_conj = nK(n, lamb)\n",
    "\n",
    "# build cost function\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized\n",
    "M = torch.rand(d, d) - 0.5 * torch.ones(d,d)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized (unitary and) block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "b_best_sorted = torch.cat([\n",
    "    torch.sort(b_best[:lamb], descending=True).values,\n",
    "    torch.sort(b_best[lamb:], descending=True).values\n",
    "]).detach().numpy()\n",
    "# print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"The numerically optimized block spectrum is \\n {b_best_sorted}\")\n",
    "print(f\"Compare with the conjectured value: \\n {n_conj.round(5)}\")\n",
    "norm_diff = np.linalg.norm(b_best_sorted - n_conj)\n",
    "print(f\"Norm of difference: \\n {norm_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e52b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.2, 0.6]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 300\n",
      "  learning_rate = 0.01\n",
      "  eps = 1e-12\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40000002 0.39999998]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.529284621679259e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.7763568394002505e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39999996 0.40000004]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 5.6811281232156754e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4 0.4]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.878662781005383e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39999998 0.40000002]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.2880658023019095e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3322676295501878e-15\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4 0.4]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 2.878662781005383e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 0.0\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function, get_b_best\n",
    "\n",
    "n = [0.2, 0.6]\n",
    "lamb = 1\n",
    "b_best_conj = nK(n, lamb)\n",
    "U_best, b_best_num, H_best, conjecture_holds = get_b_best(n, lamb, N_init=4, N_steps=300,learning_rate=0.01)\n",
    "np.allclose(b_best_conj, b_best_num, rtol=1e-6) and conjecture_holds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0645e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.2229409  0.05596309 0.1386509  0.80645666 0.91983109 0.81623408\n",
      " 0.30484322 0.99087029 0.57684608 0.68341568 0.45366076 0.33017196\n",
      " 0.80776949 0.85441828]\n",
      "  lambda = 10\n",
      "  rand_range = 1.0\n",
      "  N_init = 1\n",
      "  N_steps = 2000\n",
      "  learning_rate = 0.005\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/1\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "Gradient descent, step # 1000\n",
      "Gradient descent, step # 1100\n",
      "Gradient descent, step # 1200\n",
      "Gradient descent, step # 1300\n",
      "Gradient descent, step # 1400\n",
      "Gradient descent, step # 1500\n",
      "Gradient descent, step # 1600\n",
      "Gradient descent, step # 1700\n",
      "Gradient descent, step # 1800\n",
      "Gradient descent, step # 1900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/1\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Conjectured b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Norm of difference: \n",
      " 4.8088887725453555e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.0658141036401503e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Conjectured b_best: \n",
      " [0.80776949 0.80645666 0.68341568 0.57684608 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669 0.45366076 0.33017196 0.56053865 0.53867959\n",
      " 0.52924099 0.52341669]\n",
      "Norm of difference: \n",
      " 4.8088887725453555e-09\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.0658141036401503e-14\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n"
     ]
    }
   ],
   "source": [
    "d = 14\n",
    "lamb = 10\n",
    "n = np.random.rand(d)\n",
    "N_init = 1\n",
    "N_steps = 2000\n",
    "learning_rate = 0.005\n",
    "eps = 1e-13\n",
    "b_best_conj = nK(n, lamb)\n",
    "U_best, b_best_num, H_best, conjecture_holds = get_b_best(n, lamb, N_init=N_init, N_steps=N_steps,learning_rate=learning_rate, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3479a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 2, lambda = 1\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.57508878 0.1029472 ]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33904527 0.33899071]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 3.8581123330571574e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.321293906566325e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33904527 0.33899071]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 3.8581123330571574e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.321293906566325e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33903655 0.33899943]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 2.6243807067467882e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.536778704291919e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33903655 0.33899943]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 2.6243807067467882e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.536778704291919e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33846744 0.33956854]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 0.0007785984615642453\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3526481579262395e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33846744 0.33956854]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 0.0007785984615642453\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.3526481579262395e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33840207 0.33963391]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 0.0008710366529036511\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6928974844176992e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33903655 0.33899943]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 2.6243807067467882e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.536778704291919e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.81188253 0.11836243]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33840207 0.33963391]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 0.0008710366529036511\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6928974844176992e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.33903655 0.33899943]\n",
      "Conjectured b_best: \n",
      " [0.33901799 0.33901799]\n",
      "Norm of difference: \n",
      " 2.6243807067467882e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.536778704291919e-09\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.81188253 0.11836243]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46413026 0.4661147 ]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.001403215235180106\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.957283856603766e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46413026 0.4661147 ]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.001403215235180106\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.957283856603766e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4663301  0.46391486]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.0017078274799083104\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.861877736546006e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4663301  0.46391486]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.0017078274799083104\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.861877736546006e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46404277 0.46620218]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.0015269333434393087\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.685854855468463e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46404277 0.46620218]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.0015269333434393087\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.685854855468463e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46478104 0.46546392]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.00048287115244118864\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.686092798245056e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46478104 0.46546392]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.00048287115244118864\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.686092798245056e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.24517413 0.07913209]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46478104 0.46546392]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.00048287115244118864\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.686092798245056e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.46478104 0.46546392]\n",
      "Conjectured b_best: \n",
      " [0.46512248 0.46512248]\n",
      "Norm of difference: \n",
      " 0.00048287115244118864\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.686092798245056e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.24517413 0.07913209]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16200275 0.16230347]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00021263611483537462\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6640032463044463e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16200275 0.16230347]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00021263611483537462\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6640032463044463e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16230466 0.16200156]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00021432917144314096\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.690607075444106e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16230466 0.16200156]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00021432917144314096\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.690607075444106e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16196266 0.16234356]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00026934244553256657\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6698677069170174e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16196266 0.16234356]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00026934244553256657\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6698677069170174e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16196404 0.16234218]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.0002673891697728032\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6312842638098743e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16200275 0.16230347]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00021263611483537462\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6640032463044463e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 3, lambda = 2\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.4196277  0.48466047 0.86876905]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16196404 0.16234218]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.0002673891697728032\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.6312842638098743e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.16200275 0.16230347]\n",
      "Conjectured b_best: \n",
      " [0.16215311 0.16215311]\n",
      "Norm of difference: \n",
      " 0.00021263611483537462\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6640032463044463e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "CASE d = 3, lambda = 2\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.4196277  0.48466047 0.86876905]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64386173 0.48466006 0.64453543]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00047638262704526035\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.622352291569712e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64386173 0.48466006 0.64453543]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00047638262704526035\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.622352291569712e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64399315 0.48466045 0.64440362]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00029025112221860746\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9610531687419552e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64399315 0.48466045 0.64440362]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00029025112221860746\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9610531687419552e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64416071 0.4846601  0.6442364 ]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 5.352044578320788e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.4492307604262464e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64416071 0.4846601  0.6442364 ]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 5.352044578320788e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.4492307604262464e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64387826 0.48465968 0.64451928]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00045327371959118484\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 9.67253647177202e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64399315 0.48466045 0.64440362]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00029025112221860746\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9610531687419552e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.01569051 0.96459075 0.27959823]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64387826 0.48465968 0.64451928]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00045327371959118484\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 9.67253647177202e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.64399315 0.48466045 0.64440362]\n",
      "Conjectured b_best: \n",
      " [0.64419838 0.48466047 0.64419838]\n",
      "Norm of difference: \n",
      " 0.00029025112221860746\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9610531687419552e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.01569051 0.96459075 0.27959823]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.48940639 0.27959779 0.49087531]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.001038678682572333\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.5568634676176316e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.48940639 0.27959779 0.49087531]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.001038678682572333\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.5568634676176316e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49096064 0.27958471 0.48933415]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.001150221380800102\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4912031768510303e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49096064 0.27958471 0.48933415]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.001150221380800102\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4912031768510303e-05\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49050114 0.27959226 0.48978609]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.0005056702869614493\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.9286501719046925e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49050114 0.27959226 0.48978609]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.0005056702869614493\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 5.9286501719046925e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49004818 0.27958996 0.49024136]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.00013697316036484798\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.538336699086656e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.48940639 0.27959779 0.49087531]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.001038678682572333\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.5568634676176316e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.45288807 0.08446169 0.03144214]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.49004818 0.27958996 0.49024136]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.00013697316036484798\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.538336699086656e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.48940639 0.27959779 0.49087531]\n",
      "Conjectured b_best: \n",
      " [0.49014063 0.27959823 0.49014063]\n",
      "Norm of difference: \n",
      " 0.001038678682572333\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.5568634676176316e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.45288807 0.08446169 0.03144214]\n",
      "  lambda = 2\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 100\n",
      "  learning_rate = 0.05\n",
      "  eps = 1e-13\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24180434 0.08446164 0.24252591]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.0005102235052189052\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.666908765635583e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24180434 0.08446164 0.24252591]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.0005102235052189052\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.666908765635583e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24230913 0.08446114 0.24202162]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.00020330384113102014\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.925480329795676e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24230913 0.08446114 0.24202162]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.00020330384113102014\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.925480329795676e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24231177 0.08446167 0.24201846]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.00020740365148577748\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4923626201479578e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24231177 0.08446167 0.24201846]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.00020740365148577748\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4923626201479578e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24240652 0.08446063 0.24192475]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.0003406698239379914\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6368052717652404e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24231177 0.08446167 0.24201846]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.00020740365148577748\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4923626201479578e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24240652 0.08446063 0.24192475]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.0003406698239379914\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.6368052717652404e-06\n",
      "Conjectured majorization: \n",
      " True\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.24231177 0.08446167 0.24201846]\n",
      "Conjectured b_best: \n",
      " [0.2421651  0.08446169 0.2421651 ]\n",
      "Norm of difference: \n",
      " 0.00020740365148577748\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.4923626201479578e-07\n",
      "Conjectured majorization: \n",
      " True\n",
      "No violations to the conjecture were found.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function, get_b_best\n",
    "\n",
    "# now for a rough way how one can test conjecture across various d and lamb values. should parallelize or use cluster instead!\n",
    "d_max = 3\n",
    "conjecture_holds = True\n",
    "# TODO: make N_n, N_init, (N_steps, learning_rate) d-dependent, i.e. upgrade them to lists of length d_max - 1\n",
    "N_n = 3 # number of n vectors per choicen of (d, lamb)\n",
    "N_init = 4 # number of random initializations per n vector\n",
    "N_steps = 100\n",
    "learning_rate = 0.05\n",
    "eps = 1e-13\n",
    "for d in range(1, d_max + 1):\n",
    "    for lamb in range(int(np.ceil(d/2)), d):\n",
    "        print(F\"\\n{'='*40}\\n{'='*40}\\nCASE d = {d}, lambda = {lamb}\\n{'='*40}\\n{'='*40}\\n\")\n",
    "        for _ in range(N_n):\n",
    "            n = np.random.rand(d)\n",
    "            b_best_conj = nK(n, lamb)\n",
    "            U_best, b_best_num, H_best, conjecture_holds = get_b_best(n, lamb, N_init=N_init, N_steps=N_steps,learning_rate=learning_rate, eps=eps)\n",
    "            if conjecture_holds == False:\n",
    "                break\n",
    "        if conjecture_holds == False:\n",
    "            break\n",
    "    if conjecture_holds == False:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f02b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking some basic imports work in my environment\n",
    "\n",
    "import argparse   # to handle command-line arguments\n",
    "import numpy as np\n",
    "from scipy.stats import unitary_group\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg import block_diag\n",
    "from scipy.linalg import logm, expm\n",
    "# import qiskit\n",
    "import copy\n",
    "import scipy.special\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5b8c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cd13d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.10)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# This cell was cleared - conda commands should be run in terminal, not in Python cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ee1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaustav_conj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
