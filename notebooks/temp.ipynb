{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8655ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCalc is adding 2 + 3\n",
      "The computed sum is 5\n",
      "\n",
      "Quack! My name is Donald\n",
      "Quack!\n"
     ]
    }
   ],
   "source": [
    "# examples with classes\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store data in this instance\n",
    "    \n",
    "    def add(self, a, b):\n",
    "        print(f\"{self.name} is adding {a} + {b}\")  # Access self.name\n",
    "        return a + b\n",
    "\n",
    "# Usage:\n",
    "calc = Calculator(\"MyCalc\")\n",
    "summy = calc.add(2, 3)  # Python automatically passes calc as self\n",
    "print(f\"The computed sum is {summy}\\n\")\n",
    "\n",
    "class Duck:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store the duck's name\n",
    "    \n",
    "    def introduce(self):\n",
    "        print(f\"Quack! My name is {self.name}\")\n",
    "    \n",
    "    def quack(self):\n",
    "        print(\"Quack!\")\n",
    "\n",
    "# Usage examples:\n",
    "donald = Duck(\"Donald\")\n",
    "donald.introduce()\n",
    "donald.quack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1.009999999975, -1.9900000000026317, 4.99000000001, 3.1300000000159236], cost function = 52.8596\n",
      "x = [0.005897630008671893, -0.2904937204018134, 0.13457109604084116, 0.001668087453219903], cost function = 0.026028579594744642\n",
      "x = [0.005897630008671893, -0.2904937204018134, 0.13457109604084116, 0.001668087453219903], cost function = 0.026028579594744642\n",
      "x = [0.00037449115354536817, -0.16550934894657934, 3.226168500859898e-05, 3.5483309608153823e-12], cost function = 0.0007721943108706685\n",
      "x = [0.00037449115354536817, -0.16550934894657934, 3.226168500859898e-05, 3.5483309608153823e-12], cost function = 0.0007721943108706685\n",
      "x = [5.4498576897916806e-05, -0.11256823123990456, 2.5927969375248464e-12, 7.114101782273717e-33], cost function = 0.00016282688486587434\n",
      "x = [5.4498576897916806e-05, -0.11256823123990456, 2.5927969375248464e-12, 7.114101782273717e-33], cost function = 0.00016282688486587434\n",
      "x = [1.097225479559197e-05, -0.08172503814583272, -3.4529096537761817e-28, 1.122277847165345e-55], cost function = 4.496092716242442e-05\n",
      "x = [1.097225479559197e-05, -0.08172503814583272, -3.4529096537761817e-28, 1.122277847165345e-55], cost function = 4.496092716242442e-05\n",
      "x = [2.5850164314183053e-06, -0.06122064525186035, 4.1293257701407547e-51, 2.185807862584084e-78], cost function = 1.4115670458544878e-05\n",
      "x = [2.5850164314183053e-06, -0.06122064525186035, 4.1293257701407547e-51, 2.185807862584084e-78], cost function = 1.4115670458544878e-05\n",
      "x = [6.622443599310257e-07, -0.04663116760110854, 5.564185457147956e-74, 1.3905268387082256e-101], cost function = 4.743645877179094e-06\n",
      "x = [6.622443599310257e-07, -0.04663116760110854, 5.564185457147956e-74, 1.3905268387082256e-101], cost function = 4.743645877179094e-06\n",
      "x = [1.777910982729993e-07, -0.03585051583371858, -6.917299300598905e-97, -5.502812919536767e-124], cost function = 1.655735660473815e-06\n",
      "x = [1.777910982729993e-07, -0.03585051583371858, -6.917299300598905e-97, -5.502812919536767e-124], cost function = 1.655735660473815e-06\n",
      "x = [4.90287009399437e-08, -0.027709462286658784, -1.278435433395997e-119, -6.441845560901762e-147], cost function = 5.90595590587046e-07\n",
      "x = [4.90287009399437e-08, -0.027709462286658784, -1.278435433395997e-119, -6.441845560901762e-147], cost function = 5.90595590587046e-07\n",
      "x = [1.3732527485209696e-08, -0.021483398961054796, 1.345962948601753e-142, 8.115096883895434e-170], cost function = 2.133303908361589e-07\n",
      "x = [1.3732527485209696e-08, -0.021483398961054796, 1.345962948601753e-142, 8.115096883895434e-170], cost function = 2.133303908361589e-07\n"
     ]
    }
   ],
   "source": [
    "# gradient descent basic example.\n",
    "# Not clear how \"non-analytical\" functions can get until torch stops working...\n",
    "# copilot claims torch ONLY uses analytical stuff, even for eigenvalues...??\n",
    "# general goot practice: prefer torch functions\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#possible cost functions\n",
    "def cost_function_1(x):\n",
    "    return x[0]**2 + x[1]**4 + (x[0] + x[1])**6 + x[2]**2 + x[3]**2\n",
    "\n",
    "def cost_function_2(x): # with eigenvalues involved\n",
    "    # Reshape x into a matrix\n",
    "    A = x.view(2, 2)  # If x has 4 elements\n",
    "    eigenvals = torch.linalg.eigvals(A)\n",
    "    eigenvals_real = torch.real(eigenvals)\n",
    "    return torch.sum(eigenvals_real**2)\n",
    "\n",
    "def cost_function_3(x):\n",
    "    return max(x)\n",
    "\n",
    "d = 2\n",
    "cost_function = cost_function_1\n",
    "# define variable to be optimized\n",
    "x = torch.tensor([1.0, -2.0, 5., 3.14], dtype=torch.double, requires_grad=True)\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 10000\n",
    "optimizer = torch.optim.Adam([x], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(x)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"x = {x.tolist()}, cost function = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3820bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 1000\n",
      "Step # 2000\n",
      "Step # 3000\n",
      "\n",
      " The optimized unitary is \n",
      " tensor([[ 0.7049-0.0557j, -0.6732-0.2165j],\n",
      "        [ 0.6198-0.3403j,  0.7027-0.0791j]], dtype=torch.complex128) \n",
      " \n",
      "Absolute values of its entries: \n",
      " tensor([[0.7071, 0.7071],\n",
      "        [0.7071, 0.7071]], dtype=torch.float64) \n",
      " \n",
      "Compare to Hadamard: \n",
      " tensor([[ 0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]], dtype=torch.float64) \n",
      " \n",
      "The optimized block spectrum is \n",
      " tensor([0.3000, 0.3000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function\n",
    "\n",
    "n = [0.2, 0.4]\n",
    "lamb = 1\n",
    "# optimal rotation is e.g. a hadamard, yielding maximally mixed block spectrum [0.3, 0.3]\n",
    "# so optimal M should be s.t. M_to_A(M) = log(hadamard). here is one possibility:\n",
    "# M_optimal = torch.tensor([[ 0.4601,  0.0000], [-1.1107,  2.6815]], dtype=torch.double, requires_grad=True)\n",
    "\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized: here are some choices\n",
    "# M = torch.tensor([[0., 0.1], [0., 0.]], dtype=torch.double, requires_grad=True) # start with U0 close to identity\n",
    "# M = M_optimal\n",
    "M = torch.rand(2, 2) - 0.5 * torch.ones(2,2)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 3001\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized unitary and block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "H = torch.tensor([[1., 1.], [1., -1.]]/np.sqrt(2), dtype=torch.double)\n",
    "print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"Absolute values of its entries: \\n {torch.abs(U_best).data} \\n \")\n",
    "print(f\"Compare to Hadamard: \\n {H.data} \\n \")\n",
    "print(f\"The optimized block spectrum is \\n {b_best.data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 1000\n",
      "Step # 2000\n",
      "Step # 3000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "round() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, *, int decimals, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n\u001b[32m     28\u001b[39m b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m b_best_sorted = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_best\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_best\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe optimized block spectrum is \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_best_sorted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: round() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, *, int decimals, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "d = 6\n",
    "n = [0.1, 0.2, 0.3, 0.32, 0.6, 0.9]\n",
    "lamb = 4\n",
    "n_conj = nK(n, lamb)\n",
    "\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized\n",
    "M = torch.rand(d, d) - 0.5 * torch.ones(d,d)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 3001\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized (unitary and) block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "b_best_sorted = torch.round(torch.cat([\n",
    "    torch.sort(b_best[:lamb], descending=True).values,\n",
    "    torch.sort(b_best[lamb:], descending=True).values\n",
    "]), decimals=5).tolist()\n",
    "# print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"The optimized block spectrum is \\n {b_best_sorted}\")\n",
    "print(f\"Compare with the conjectured value: \\n {n_conj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03f8f335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.4000, dtype=torch.float64),\n",
       " tensor(0.3200, dtype=torch.float64),\n",
       " tensor(0.3000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.4000, dtype=torch.float64)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_conj = nK(torch.tensor(n, dtype=torch.double), lamb)\n",
    "n_conj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52b711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaustav_conj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
