{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8655ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCalc is adding 2 + 3\n",
      "The computed sum is 5\n",
      "\n",
      "Quack! My name is Donald\n",
      "Quack!\n"
     ]
    }
   ],
   "source": [
    "# examples with classes\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store data in this instance\n",
    "    \n",
    "    def add(self, a, b):\n",
    "        print(f\"{self.name} is adding {a} + {b}\")  # Access self.name\n",
    "        return a + b\n",
    "\n",
    "# Usage:\n",
    "calc = Calculator(\"MyCalc\")\n",
    "summy = calc.add(2, 3)  # Python automatically passes calc as self\n",
    "print(f\"The computed sum is {summy}\\n\")\n",
    "\n",
    "class Duck:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Store the duck's name\n",
    "    \n",
    "    def introduce(self):\n",
    "        print(f\"Quack! My name is {self.name}\")\n",
    "    \n",
    "    def quack(self):\n",
    "        print(\"Quack!\")\n",
    "\n",
    "# Usage examples:\n",
    "donald = Duck(\"Donald\")\n",
    "donald.introduce()\n",
    "donald.quack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9597f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1.009999999975, -1.9900000000026317, 4.99000000001, 3.1300000000159236], cost function = 52.8596\n",
      "x = [0.005897630008671893, -0.2904937204018134, 0.13457109604084116, 0.001668087453219903], cost function = 0.026028579594744642\n",
      "x = [0.00037449115354536817, -0.16550934894657934, 3.226168500859898e-05, 3.5483309608153823e-12], cost function = 0.0007721943108706685\n",
      "x = [5.4498576897916806e-05, -0.11256823123990456, 2.5927969375248464e-12, 7.114101782273717e-33], cost function = 0.00016282688486587434\n",
      "x = [1.097225479559197e-05, -0.08172503814583272, -3.4529096537761817e-28, 1.122277847165345e-55], cost function = 4.496092716242442e-05\n",
      "x = [2.5850164314183053e-06, -0.06122064525186035, 4.1293257701407547e-51, 2.185807862584084e-78], cost function = 1.4115670458544878e-05\n",
      "x = [6.622443599310257e-07, -0.04663116760110854, 5.564185457147956e-74, 1.3905268387082256e-101], cost function = 4.743645877179094e-06\n",
      "x = [1.777910982729993e-07, -0.03585051583371858, -6.917299300598905e-97, -5.502812919536767e-124], cost function = 1.655735660473815e-06\n",
      "x = [4.90287009399437e-08, -0.027709462286658784, -1.278435433395997e-119, -6.441845560901762e-147], cost function = 5.90595590587046e-07\n",
      "x = [1.3732527485209696e-08, -0.021483398961054796, 1.345962948601753e-142, 8.115096883895434e-170], cost function = 2.133303908361589e-07\n"
     ]
    }
   ],
   "source": [
    "# gradient descent basic example.\n",
    "# Not clear how \"non-analytical\" functions can get until torch stops working...\n",
    "# copilot claims torch ONLY uses analytical stuff, even for eigenvalues...??\n",
    "# general goot practice: prefer torch functions\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#possible cost functions\n",
    "def cost_function_1(x):\n",
    "    return x[0]**2 + x[1]**4 + (x[0] + x[1])**6 + x[2]**2 + x[3]**2\n",
    "\n",
    "def cost_function_2(x): # with eigenvalues involved\n",
    "    # Reshape x into a matrix\n",
    "    A = x.view(2, 2)  # If x has 4 elements\n",
    "    eigenvals = torch.linalg.eigvals(A)\n",
    "    eigenvals_real = torch.real(eigenvals)\n",
    "    return torch.sum(eigenvals_real**2)\n",
    "\n",
    "def cost_function_3(x):\n",
    "    return max(x)\n",
    "\n",
    "d = 2\n",
    "cost_function = cost_function_1\n",
    "# define variable to be optimized\n",
    "x = torch.tensor([1.0, -2.0, 5., 3.14], dtype=torch.double, requires_grad=True)\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 10000\n",
    "optimizer = torch.optim.Adam([x], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(x)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"x = {x.tolist()}, cost function = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 1000\n",
      "Step # 2000\n",
      "Step # 3000\n",
      "\n",
      " The optimized unitary is \n",
      " tensor([[ 0.6920+0.1455j,  0.5108-0.4890j],\n",
      "        [-0.3043-0.6383j,  0.6986+0.1092j]], dtype=torch.complex128) \n",
      " \n",
      "Absolute values of its entries: \n",
      " tensor([[0.7071, 0.7071],\n",
      "        [0.7071, 0.7071]], dtype=torch.float64) \n",
      " \n",
      "Compare to Hadamard: \n",
      " tensor([[ 0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]], dtype=torch.float64) \n",
      " \n",
      "The optimized block spectrum is \n",
      " tensor([0.3000, 0.3000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function\n",
    "\n",
    "\n",
    "n = [0.2, 0.4]\n",
    "lamb = 1\n",
    "# optimal rotation is e.g. a hadamard, yielding maximally mixed block spectrum [0.3, 0.3]\n",
    "# so optimal M should be s.t. M_to_A(M) = log(hadamard). here is one possibility:\n",
    "# M_optimal = torch.tensor([[ 0.4601,  0.0000], [-1.1107,  2.6815]], dtype=torch.double, requires_grad=True)\n",
    "\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized: here are some choices\n",
    "# M = torch.tensor([[0., 0.1], [0., 0.]], dtype=torch.double, requires_grad=True) # start with U0 close to identity\n",
    "# M = M_optimal\n",
    "M = torch.rand(2, 2) - 0.5 * torch.ones(2,2)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "n_steps = 3001\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized unitary and block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "H = torch.tensor([[1., 1.], [1., -1.]]/np.sqrt(2), dtype=torch.double)\n",
    "print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"Absolute values of its entries: \\n {torch.abs(U_best).data} \\n \")\n",
    "print(f\"Compare to Hadamard: \\n {H.data} \\n \")\n",
    "print(f\"The optimized block spectrum is \\n {b_best.data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4883994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 0\n",
      "Step # 100\n",
      "Step # 200\n",
      "Step # 300\n",
      "Step # 400\n",
      "Step # 500\n",
      "Step # 600\n",
      "Step # 700\n",
      "Step # 800\n",
      "Step # 900\n",
      "Step # 1000\n",
      "The numerically optimized block spectrum is \n",
      " [0.60126569 0.52562717 0.49598848 0.47878389 0.3906772  0.52562717\n",
      " 0.49598848 0.47878389]\n",
      "Compare with the conjectured value: \n",
      " [0.60127 0.52563 0.49599 0.47878 0.39068 0.52563 0.49599 0.47878]\n",
      "Norm of difference: \n",
      " 1.877551628332819e-09\n"
     ]
    }
   ],
   "source": [
    "d = 8\n",
    "n = np.random.rand(d)\n",
    "lamb = 5\n",
    "learning_rate = 0.01\n",
    "n_steps = 1001\n",
    "\n",
    "# get conjectured bbest\n",
    "n_conj = nK(n, lamb)\n",
    "\n",
    "# build cost function\n",
    "cost_function = build_cost_function(n, lamb)\n",
    "\n",
    "# initialize variable to be optimized\n",
    "M = torch.rand(d, d) - 0.5 * torch.ones(d,d)\n",
    "M.requires_grad_(True)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([M], lr=learning_rate)\n",
    "for step in range(n_steps):\n",
    "    optimizer.zero_grad() # clear previous gradient\n",
    "    loss = cost_function(M)\n",
    "    loss.backward() # compute gradient\n",
    "    optimizer.step() # update x\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step # {step}\")\n",
    "#        print(f\"M = {M.tolist()}, cost function = {loss}\")\n",
    "\n",
    "# print optimized (unitary and) block spec\n",
    "U_best = torch.matrix_exp(M_to_A(M))\n",
    "D = torch.diag(torch.tensor(n, dtype=torch.cdouble))\n",
    "b_best = torch.real(block_spec(U_best @ D @ U_best.adjoint(), lamb))\n",
    "b_best_sorted = torch.cat([\n",
    "    torch.sort(b_best[:lamb], descending=True).values,\n",
    "    torch.sort(b_best[lamb:], descending=True).values\n",
    "]).detach().numpy()\n",
    "# print(f\"\\n The optimized unitary is \\n {U_best.data} \\n \")\n",
    "print(f\"The numerically optimized block spectrum is \\n {b_best_sorted}\")\n",
    "print(f\"Compare with the conjectured value: \\n {n_conj.round(5)}\")\n",
    "norm_diff = np.linalg.norm(b_best_sorted - n_conj)\n",
    "print(f\"Norm of difference: \\n {norm_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e52b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.2, 0.6]\n",
      "  lambda = 1\n",
      "  rand_range = 1.0\n",
      "  N_init = 4\n",
      "  N_steps = 300\n",
      "  learning_rate = 0.01\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40000001 0.39999999]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 1.252108462928746e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.661338147750939e-16\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.39999997 0.40000003]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 4.1488198571202714e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 3.774758283725532e-15\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.40000004 0.39999996]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 5.430040997969318e-08\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 6.439293542825908e-15\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/4\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/4\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4 0.4]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 8.860525538431112e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.4 0.4]\n",
      "Conjectured b_best: \n",
      " [0.4 0.4]\n",
      "Norm of difference: \n",
      " 8.860525538431112e-10\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.220446049250313e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For Jupyter notebooks, use \"relative path\"\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from kaustav_conj.utils import h, H, nK, block_spec, M_to_A\n",
    "from kaustav_conj.core import build_cost_function, get_b_best\n",
    "\n",
    "n = [0.2, 0.6]\n",
    "lamb = 1\n",
    "b_best_conj = nK(n, lamb)\n",
    "U_best, b_best_num, H_best = get_b_best(n, lamb, N_init=4, N_steps=300,learning_rate=0.01)\n",
    "np.allclose(b_best_conj, b_best_num, rtol=1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0645e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Starting get_b_best optimization\n",
      "========================================\n",
      "Parameters recap:\n",
      "  n = [0.37492587 0.13857947 0.93255657 0.53490564 0.16198984 0.27835791\n",
      " 0.1842687  0.72452978 0.9737716  0.15680316 0.19674044 0.24392199\n",
      " 0.88963693 0.31072097]\n",
      "  lambda = 10\n",
      "  rand_range = 1.0\n",
      "  N_init = 5\n",
      "  N_steps = 1000\n",
      "  learning_rate = 0.01\n",
      "\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 1/5\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 1/5\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55617594 0.54471195 0.53490565 0.52578025 0.45439548 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617528 0.5447112\n",
      " 0.52578295 0.454403  ]\n",
      "Conjectured b_best: \n",
      " [0.55617553 0.54467986 0.53490564 0.52581339 0.45439924 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617553 0.54467986\n",
      " 0.52581339 0.45439924]\n",
      "Norm of difference: \n",
      " 6.374932705053272e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 4.8458234775949904e-06\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 2/5\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 2/5\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55617538 0.54515726 0.53490565 0.52533018 0.45440138 0.37492586\n",
      " 0.31072097 0.2783579  0.24392199 0.19674043 0.55617647 0.54515155\n",
      " 0.52534704 0.45439681]\n",
      "Conjectured b_best: \n",
      " [0.55617553 0.54467986 0.53490564 0.52581339 0.45439924 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617553 0.54467986\n",
      " 0.52581339 0.45439924]\n",
      "Norm of difference: \n",
      " 0.0009494064566695097\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 7.401802436213245e-05\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 3/5\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 3/5\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55617578 0.5448267  0.53490566 0.52566514 0.45440018 0.37492586\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617685 0.54482489\n",
      " 0.52566821 0.45439827]\n",
      "Conjectured b_best: \n",
      " [0.55617553 0.54467986 0.53490564 0.52581339 0.45439924 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617553 0.54467986\n",
      " 0.52581339 0.45439924]\n",
      "Norm of difference: \n",
      " 0.00029266246547034007\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 2.2517943270017327e-05\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 4/5\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 4/5\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55617601 0.54469377 0.53490564 0.5258008  0.45439878 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617568 0.54468993\n",
      " 0.52580138 0.4543997 ]\n",
      "Conjectured b_best: \n",
      " [0.55617553 0.54467986 0.53490564 0.52581339 0.45439924 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617553 0.54467986\n",
      " 0.52581339 0.45439924]\n",
      "Norm of difference: \n",
      " 2.4453587407409028e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9000533519886176e-06\n",
      "\n",
      "====================\n",
      "Starting gradient descent run 5/5\n",
      "====================\n",
      "Gradient descent, step # 0\n",
      "Gradient descent, step # 100\n",
      "Gradient descent, step # 200\n",
      "Gradient descent, step # 300\n",
      "Gradient descent, step # 400\n",
      "Gradient descent, step # 500\n",
      "Gradient descent, step # 600\n",
      "Gradient descent, step # 700\n",
      "Gradient descent, step # 800\n",
      "Gradient descent, step # 900\n",
      "\n",
      "====================\n",
      "Results of gradient descent run 5/5\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55617841 0.54475329 0.53490565 0.52573865 0.45439831 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617427 0.54475018\n",
      " 0.52574277 0.45440016]\n",
      "Conjectured b_best: \n",
      " [0.55617553 0.54467986 0.53490564 0.52581339 0.45439924 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617553 0.54467986\n",
      " 0.52581339 0.45439924]\n",
      "Norm of difference: \n",
      " 0.00014463118732127068\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.1153861926516129e-05\n",
      "\n",
      "========================================\n",
      "get_b_best optimization finished\n",
      "========================================\n",
      "\n",
      "====================\n",
      " Final results, optimized over various initializations\n",
      "====================\n",
      "Numerical b_best: \n",
      " [0.55617601 0.54469377 0.53490564 0.5258008  0.45439878 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617568 0.54468993\n",
      " 0.52580138 0.4543997 ]\n",
      "Conjectured b_best: \n",
      " [0.55617553 0.54467986 0.53490564 0.52581339 0.45439924 0.37492587\n",
      " 0.31072097 0.27835791 0.24392199 0.19674044 0.55617553 0.54467986\n",
      " 0.52581339 0.45439924]\n",
      "Norm of difference: \n",
      " 2.4453587407409028e-05\n",
      "Conjectured H_best - numerical H_best (should be > 0): \n",
      " 1.9000533519886176e-06\n"
     ]
    }
   ],
   "source": [
    "d = 14\n",
    "lamb = 10\n",
    "n = np.random.rand(d)\n",
    "N_init = 15\n",
    "N_steps = 4000\n",
    "learning_rate = 0.005\n",
    "b_best_conj = nK(n, lamb)\n",
    "U_best, b_best_num, H_best = get_b_best(n, lamb, N_init=N_init, N_steps=N_steps,learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92177a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaustav_conj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
